{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yl3394/GMMDA/blob/main/gmmda_coauthor_physics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToaaY0zPyRJY"
      },
      "source": [
        "# GMMDA for Coauthor-Physcis Dataset\n",
        "## Section 1: Initilization Section for GNN\n",
        "This section we defined necessary functions and classes for training and evluating the GMM-based auxiliary learning GCN for the graph data augmentation task. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyxxMmjloTVn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "#os.environ['TORCH'] = torch.__version__\n",
        "os.environ['TORCH'] = '1.13.0+cu116' # now the system is 1.13.1+cu116, but no precompiled wheel of it\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTZdnuNHrkLY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Connect to Google Drive \n",
        "ds = 'coauthor-physics'\n",
        "# If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "save_to_gdrive = True \n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Enter the directory name to save model at.\n",
        "OUTPUT_DIR = f\"gda_dnml/{ds}\" \n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "!mkdir -p $OUTPUT_DIR\n",
        "\n",
        "DATA_DIR = OUTPUT_DIR + '/data'\n",
        "print(f\"[*] Data will be saved at {DATA_DIR}\")\n",
        "!mkdir -p $DATA_DIR\n",
        "\n",
        "MODEL_DIR = OUTPUT_DIR + '/model'\n",
        "print(f\"[*] Model will be saved at {DATA_DIR}\")\n",
        "!mkdir -p $MODEL_DIR\n",
        "\n",
        "EXP_RESULT_DIR = OUTPUT_DIR + '/output'\n",
        "print(f\"[*] Experimental results will be saved at {EXP_RESULT_DIR}\")\n",
        "!mkdir -p $EXP_RESULT_DIR\n",
        "\n",
        "FIGURE_DIR = OUTPUT_DIR + '/figure'\n",
        "print(f\"[*] Image will be saved at {FIGURE_DIR}\")\n",
        "!mkdir -p $FIGURE_DIR\n",
        "\n",
        "FIGURE_ABLATION_DIR = FIGURE_DIR + '/ablation_analysis'\n",
        "print(f\"[*] Image of ablation analysis will be saved at {FIGURE_ABLATION_DIR}\")\n",
        "!mkdir -p $FIGURE_ABLATION_DIR\n",
        "\n",
        "# make sub-directories for analysis \n",
        "c_analysis_dir = FIGURE_ABLATION_DIR + '/candidate_analysis'\n",
        "tsne_dnml_dir = c_analysis_dir + '/dnml_tsne_best_emb'\n",
        "!mkdir -p $c_analysis_dir\n",
        "!mkdir -p $tsne_dnml_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUjDQPWoLdbi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load Data to Get Number of Components\n",
        "import pickle \n",
        "\n",
        "with open(f\"/content/drive/MyDrive/gda_dnml/{ds}/data/{ds}-preprocessed-data\", 'rb') as fp:\n",
        "    data = pickle.load(fp)\n",
        "\n",
        "with open(f\"/content/drive/MyDrive/gda_dnml/{ds}/data/{ds}-preprocessed-dataset\", 'rb') as fp:\n",
        "    dataset = pickle.load(fp)\n",
        "\n",
        "num_classes = dataset.num_classes\n",
        "\n",
        "print(f\"\"\"{ds}:\n",
        "  Size of augmented training set: {data.train_mask.sum()}, size of original training set: {data.train_mask.sum()}; \n",
        "  Size of augmented test set: {data.test_mask.sum()}, size of original test set: {data.test_mask.sum()};\n",
        "  Size of augmented validation set: {data.val_mask.sum()}, size of original validation set: {data.val_mask.sum()}.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2-y55pxrVBD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title GNN Code \n",
        "##### import sys\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import csr_matrix\n",
        "import gc \n",
        "\n",
        "class GNN(object):\n",
        "    \"\"\"Graph Neural Networks that can be easily called and used.\n",
        "\n",
        "    Authors of this code package:\n",
        "    Tong Zhao, tzhao2@nd.edu\n",
        "    Tianwen Jiang, twjiang@ir.hit.edu.cn\n",
        "\n",
        "    Last updated: 11/25/2019\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    adj_matrix: scipy.sparse.csr_matrix\n",
        "        The adjacency matrix of the graph, where nonzero entries indicates edges.\n",
        "        The number of each nonzero entry indicates the number of edges between these two nodes.\n",
        "\n",
        "    features: numpy.ndarray, optional\n",
        "        The 2-dimension np array that stores given raw feature of each node, where the i-th row\n",
        "        is the raw feature vector of node i.\n",
        "        When raw features are not given, one-hot degree features will be used.\n",
        "\n",
        "    labels: list or 1-D numpy.ndarray, optional\n",
        "        The class label of each node. Used for supervised learning.\n",
        "        \n",
        "    learn_method: {'unsup', 'sup', 'aux'}, defualt 'unsup'\n",
        "        Whether to use supervised learning, unsupervised learning or supervised+unsupervised (auxiliry) learning.\n",
        "\n",
        "    model: {'gat', 'graphsage'}, default 'gat'\n",
        "        The GNN model to be used.\n",
        "        - 'graphsage' is GraphSAGE: https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n",
        "        - 'gat' is graph attention network: https://arxiv.org/pdf/1710.10903.pdf\n",
        "\n",
        "    n_layer: int, optional, default 2\n",
        "        Number of layers in the GNN\n",
        "\n",
        "    emb_size: int, optional, default 128\n",
        "        Size of the node embeddings to be learnt\n",
        "\n",
        "    random_state, int, optional, default 1234\n",
        "        Random seed\n",
        "\n",
        "    device: {'cpu', 'cuda', 'auto'}, default 'auto'\n",
        "        The device to use.\n",
        "\n",
        "    epochs: int, optional, default 5\n",
        "        Number of epochs for training\n",
        "\n",
        "    batch_size: int, optional, default 20\n",
        "        Number of node per batch for training\n",
        "\n",
        "    lr: float, optional, default 0.7\n",
        "        Learning rate\n",
        "\n",
        "    unsup_loss_type: {'margin', 'normal'}, default 'margin'\n",
        "        Loss function to be used for unsupervised learning\n",
        "        - 'margin' is a hinge loss with margin of 3\n",
        "        - 'normal' is the unsupervised loss function described in the paper of GraphSAGE\n",
        "\n",
        "    print_progress: bool, optional, default True\n",
        "        Whether to print the training progress\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 adj_matrix, \n",
        "                 features=None, \n",
        "                 labels=None, \n",
        "                 learn_method = 'unsup',\n",
        "                 model='gat', \n",
        "                 n_layer=2, \n",
        "                 emb_size=64, \n",
        "                 random_state=1234, \n",
        "                 device='auto', \n",
        "                 epochs=5, \n",
        "                 batch_size=20, \n",
        "                 lr=0.7, \n",
        "                 unsup_loss_type='margin', \n",
        "                 print_progress=True):\n",
        "        super(GNN, self).__init__()\n",
        "        # fix random seeds\n",
        "        random.seed(random_state)\n",
        "        np.random.seed(random_state)\n",
        "        torch.manual_seed(random_state)\n",
        "        torch.cuda.manual_seed_all(random_state)\n",
        "        # set parameters\n",
        "        self.learn_method = learn_method            \n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.unsup_loss_type = unsup_loss_type\n",
        "        self.print_progress = print_progress\n",
        "        self.gat = False\n",
        "        self.gcn = False\n",
        "        if model == 'gat':\n",
        "            self.gat = True\n",
        "            self.model_name = 'GAT'\n",
        "        elif model == 'gcn':\n",
        "            self.gcn = True\n",
        "            self.model_name = 'GCN'\n",
        "        else:\n",
        "            self.model_name = 'GraphSAGE'\n",
        "        # set device\n",
        "        if device == 'auto':\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        # load data\n",
        "        self.dl = DataLoader(adj_matrix, features, labels, learn_method, self.device)\n",
        "\n",
        "        self.gnn = GNN_model(n_layer, emb_size, self.dl, self.device, gat=self.gat, gcn=self.gcn)\n",
        "        self.gnn.to(self.device)\n",
        "\n",
        "        if learn_method != 'unsup':\n",
        "            n_classes = len(set(labels))\n",
        "            self.classification = Classification(emb_size, n_classes)\n",
        "            self.classification.to(self.device)\n",
        "        \n",
        "        print(f'K has been set to = {num_classes}')\n",
        "        self.gmm = GaussianMixtureModel(n_components = num_classes)\n",
        "        self.gmm.to(self.device)\n",
        "            \n",
        "    def fit(self):\n",
        "        train_nodes = copy.deepcopy(self.dl.nodes_train)                \n",
        "                \n",
        "        if self.learn_method == 'sup':\n",
        "            # superivsed learning\n",
        "            labels = self.dl.labels\n",
        "            models = [self.gnn, self.classification]\n",
        "        elif self.learn_method == 'aux':\n",
        "            # superivsed learning\n",
        "            labels = self.dl.labels\n",
        "            models = [self.gnn, self.classification]\n",
        "            # unsuperivsed learning\n",
        "            unsup_loss = Unsup_Loss(self.dl, self.device)\n",
        "            if self.unsup_loss_type == 'margin':\n",
        "                num_neg = 6\n",
        "            elif self.unsup_loss_type == 'normal':\n",
        "                num_neg = 100\n",
        "        else:\n",
        "            unsup_loss = Unsup_Loss(self.dl, self.device)\n",
        "            models = [self.gnn]\n",
        "            if self.unsup_loss_type == 'margin':\n",
        "                num_neg = 6\n",
        "            elif self.unsup_loss_type == 'normal':\n",
        "                num_neg = 100\n",
        "\n",
        "        print(f\"Initializing a GNN... with shape {train_nodes.shape}\")\n",
        "        nodes_batch_pretrain = train_nodes\n",
        "        # extend nodes batch for unspervised learning\n",
        "        nodes_batch_pretrain = np.asarray(list(unsup_loss.extend_nodes(nodes_batch_pretrain, num_neg=num_neg)))\n",
        "        # print(f\"nodes_batch_pretrain unique {len(np.unique(nodes_batch_pretrain))}\")\n",
        "        # feed nodes batch to the GNN and returning the nodes embeddings                \n",
        "        gnn_pretrain_optimizer = torch.optim.SGD(self.gnn.parameters(), lr=self.lr)\n",
        "        classification_pretrain_optimizer = torch.optim.SGD(self.classification.parameters(), lr=self.lr)\n",
        "        \n",
        "        # add pre-train O_1 \n",
        "        for _ in range(200): #200\n",
        "            # clean up cache\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            \n",
        "            embs_batch_pretrain = self.gnn(nodes_batch_pretrain)\n",
        "            #print(f'outputing embedding pretrain with shape {embs_batch_pretrain.shape}, nodes_batch_pretrain: {nodes_batch_pretrain.shape}')\n",
        "            \n",
        "            loss_net_pretrain = unsup_loss.get_loss_margin(embs_batch_pretrain, nodes_batch_pretrain)\n",
        "            \n",
        "            logists_pretrain = self.classification(embs_batch_pretrain)\n",
        "            labels_batch = labels[nodes_batch_pretrain]\n",
        "            loss_sup_pretrain = -torch.sum(logists_pretrain[range(logists_pretrain.size(0)), labels_batch], 0)\n",
        "            loss_sup_pretrain /= len(nodes_batch_pretrain)\n",
        "            \n",
        "            loss_pretrain = loss_sup_pretrain + loss_net_pretrain\n",
        "            \n",
        "            # clean up cache\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            \n",
        "            loss_pretrain.backward(retain_graph=True)\n",
        "            if _ % 50 == 0:\n",
        "                print(f\"GNN pre-training step {_}th iteration's loss = {loss_net_pretrain}\")\n",
        "            gnn_pretrain_optimizer.step()\n",
        "            gnn_pretrain_optimizer.zero_grad()\n",
        "            \n",
        "            classification_pretrain_optimizer.step()\n",
        "            classification_pretrain_optimizer.zero_grad() \n",
        "    \n",
        "        print(f'outputing embedding pretrain with shape {embs_batch_pretrain.shape}')\n",
        "                \n",
        "        for epoch in range(self.epochs):\n",
        "            np.random.shuffle(train_nodes)\n",
        "            \n",
        "            # pretrain GMM for a better initialization\n",
        "            # print('Initializing a pre-trained GMM ...')\n",
        "            embs_all = self.gnn(train_nodes)\n",
        "            gmm_optimizer = torch.optim.SGD(self.gmm.parameters(), lr=0.0000001, momentum=0.9) # lr=0.000001, momentum=0.8)\n",
        "\n",
        "            # I_2\n",
        "            for _ in range(5_00): #1_000\n",
        "                gmm_pretrain_loss = self.gmm(embs_all)\n",
        "                gmm_pretrain_loss.backward(retain_graph=True)\n",
        "                if _ % 500 == 0:\n",
        "                    print(f\"GMM training step {_}th iteration's loss = {gmm_pretrain_loss}\")\n",
        "                gmm_optimizer.step()\n",
        "                if torch.all(self.gmm._get_weights() >= 0) == True: \n",
        "                    gmm_optimizer.zero_grad()\n",
        "                else: \n",
        "                    break \n",
        "                # clean up cache\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                \n",
        "            weights, means, stdevs = self.gmm._get_parameters()\n",
        "\n",
        "            # update GMM parameters\n",
        "            self.gmm._update_parameters(weights, means, stdevs)\n",
        "            loss_gmm = self.gmm(embs_all)\n",
        "            loss_gmm /= len(train_nodes)\n",
        "\n",
        "            params = []\n",
        "            \n",
        "            for model in models:\n",
        "                \n",
        "                \"\"\"for name, param in model.named_parameters():\n",
        "                    if param.requires_grad:\n",
        "                        print(name, param.data)\"\"\"\n",
        "                \n",
        "                for param in model.parameters():\n",
        "                    if param.requires_grad:\n",
        "                        params.append(param)\n",
        "            optimizer = torch.optim.SGD(params, lr=self.lr)\n",
        "            optimizer.zero_grad()\n",
        "            for model in models:\n",
        "                model.zero_grad()\n",
        "\n",
        "            batches = math.ceil(len(train_nodes) / self.batch_size)\n",
        "            visited_nodes = set()\n",
        "            if self.print_progress:\n",
        "                tqdm_bar = tqdm(range(batches), ascii=True, leave=False)\n",
        "            else:\n",
        "                tqdm_bar = range(batches)\n",
        "                \n",
        "            # I_3\n",
        "            for i3 in range(300):  #300\n",
        "                for index in tqdm_bar:\n",
        "                    if self.learn_method != 'sup' and len(visited_nodes) == len(train_nodes):\n",
        "                        # finish this epoch if all nodes are visited\n",
        "                        if self.print_progress:\n",
        "                            tqdm_bar.close()\n",
        "                        break\n",
        "                    nodes_batch = train_nodes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "                    # extend nodes batch for unspervised learning\n",
        "                    if self.learn_method != 'sup':\n",
        "                        nodes_batch = np.asarray(list(unsup_loss.extend_nodes(nodes_batch, num_neg=num_neg)))\n",
        "                    visited_nodes |= set(nodes_batch)\n",
        "                    # feed nodes batch to the GNN and returning the nodes embeddings\n",
        "                    embs_batch = self.gnn(nodes_batch)\n",
        "\n",
        "                    # calculate loss\n",
        "                    if self.learn_method == 'sup':\n",
        "                        # superivsed learning\n",
        "                        logists = self.classification(embs_batch)\n",
        "                        labels_batch = labels[nodes_batch]\n",
        "                        loss_sup = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
        "                        loss_sup /= len(nodes_batch)\n",
        "                        loss = loss_sup \n",
        "                    elif self.learn_method == 'aux':\n",
        "                        # superivsed learning\n",
        "                        logists = self.classification(embs_batch)\n",
        "                        labels_batch = labels[nodes_batch]\n",
        "                        loss_sup = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
        "                        loss_sup /= len(nodes_batch)\n",
        "                        # unsuperivsed learning\n",
        "                        if self.unsup_loss_type == 'margin':\n",
        "                            loss_net = unsup_loss.get_loss_margin(embs_batch, nodes_batch)\n",
        "                        elif self.unsup_loss_type == 'normal':\n",
        "                            loss_net = unsup_loss.get_loss_sage(embs_batch, nodes_batch)\n",
        "                        loss = loss_sup + loss_net \n",
        "                    else:\n",
        "                        if self.unsup_loss_type == 'margin':\n",
        "                            loss_net = unsup_loss.get_loss_margin(embs_batch, nodes_batch)\n",
        "                        elif self.unsup_loss_type == 'normal':\n",
        "                            loss_net = unsup_loss.get_loss_sage(embs_batch, nodes_batch)\n",
        "                        loss = loss_net \n",
        "\n",
        "\n",
        "                    # add losses together \n",
        "                    loss = loss + loss_gmm\n",
        "\n",
        "\n",
        "                    if self.print_progress:\n",
        "                        progress_message = '{} Epoch: [{}/{}], I3 iteration [{}/100] current loss: {:.4f}, touched nodes [{}/{}] '.format(\n",
        "                                        self.model_name, epoch+1, self.epochs, i3,loss.item(), len(visited_nodes), len(train_nodes))\n",
        "                        tqdm_bar.set_description(progress_message)\n",
        "\n",
        "                    loss.backward()\n",
        "                    for model in models:\n",
        "                        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    for model in models:\n",
        "                        model.zero_grad()\n",
        "                     \n",
        "                    # clean up cache\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                    #nc_logits_eval = self.classification(embs_batch)\n",
        "                    #class_label_eval = nc_logits_eval.max(1).indices\n",
        "                    #class_label_eval = class_label_eval.cpu().detach().numpy()\n",
        "\n",
        "                    #val_acc = accuracy_score(labels, class_label_eval)\n",
        "                          \n",
        "            print('{} Epoch: [{}/{}], current loss: {:.4f}'.format(\n",
        "                self.model_name, epoch+1, self.epochs, loss.item()))\n",
        "            \n",
        "            # clean up cache\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            \n",
        "            \n",
        "    def _get_models(self):\n",
        "        return(self.gnn, self.gmm, self.classification)\n",
        "\n",
        "    def generate_embeddings(self, nodes):\n",
        "        nodes = nodes #self.dl.nodes_train\n",
        "        print(f'nodes: {nodes.shape}')\n",
        "        b_sz = 500\n",
        "        batches = math.ceil(len(nodes) / b_sz)\n",
        "        embs = []\n",
        "        for index in range(batches):\n",
        "            nodes_batch = nodes[index*b_sz:(index+1)*b_sz]\n",
        "            with torch.no_grad():\n",
        "                embs_batch = self.gnn(nodes_batch)\n",
        "            assert len(embs_batch) == len(nodes_batch)\n",
        "            embs.append(embs_batch)\n",
        "        assert len(embs) == batches\n",
        "        embs = torch.cat(embs, 0)\n",
        "        assert len(embs) == len(nodes)\n",
        "        return embs.cpu().numpy()\n",
        "\n",
        "    def predict(self, nodes):\n",
        "        if self.learn_method == 'unsup':\n",
        "            print('GNN.predict() is only supported for supervised learning.')\n",
        "            sys.exit(0)\n",
        "        nodes = self.dl.nodes_train\n",
        "        b_sz = 500\n",
        "        batches = math.ceil(len(nodes) / b_sz)\n",
        "        preds = []\n",
        "        for index in range(batches):\n",
        "            nodes_batch = nodes[index*b_sz:(index+1)*b_sz]\n",
        "            with torch.no_grad():\n",
        "                embs_batch = self.gnn(nodes_batch)\n",
        "                logists = self.classification(embs_batch)\n",
        "                _, predicts = torch.max(logists, 1)\n",
        "                preds.append(predicts)\n",
        "        assert len(preds) == batches\n",
        "        preds = torch.cat(preds, 0)\n",
        "        assert len(preds) == len(nodes)\n",
        "        return preds.cpu().numpy()\n",
        "    \n",
        "    def eval_node_cls(nc_logits, labels):\n",
        "        \"\"\" evaluate node classification results \"\"\"\n",
        "        preds = torch.argmax(nc_logits, dim=1)\n",
        "        correct = torch.sum(preds == labels)\n",
        "        acc = correct.item() / len(labels)\n",
        "        return acc\n",
        "\n",
        "    def release_cuda_cache(self):\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "class DataLoader(object):\n",
        "    def __init__(self, adj_matrix, raw_features, labels, learn_method, device):\n",
        "        super(DataLoader, self).__init__()\n",
        "        self.adj_matrix = adj_matrix\n",
        "        # load adjacency list and node features\n",
        "        self.adj_list = self.get_adj_list(adj_matrix)\n",
        "        if raw_features is None:\n",
        "            features = self.get_features()\n",
        "        else:\n",
        "            features = raw_features\n",
        "        assert features.shape[0] == len(self.adj_list) == self.adj_matrix.shape[0]\n",
        "        self.features = torch.FloatTensor(features).to(device)\n",
        "        #self.nodes_train = list(range(len(self.adj_list)))\n",
        "        \n",
        "        # split data into training, test, and validation sets temp \n",
        "        #features = torch.FloatTensor(getattr(dataCenter, ds+'_feats')).to(device)\n",
        "        #labels = torch.FloatTensor(getattr(dataCenter, ds+'_labels')).to(device)\n",
        "        #adj_lists = getattr(dataCenter, ds+'_adj_lists')\n",
        "        self.nodes_test = getattr(dataCenter, ds+'_test')\n",
        "        self.nodes_val = getattr(dataCenter, ds+'_val')\n",
        "        self.nodes_train = getattr(dataCenter, ds+'_train')\n",
        "        \n",
        "        if learn_method != 'unsup':\n",
        "            self.labels = np.asarray(labels)\n",
        "\n",
        "    def get_adj_list(self, adj_matrix):\n",
        "        \"\"\"build adjacency list from adjacency matrix\"\"\"\n",
        "        adj_list = {}\n",
        "        for i in range(adj_matrix.shape[0]):\n",
        "            adj_list[i] = set(np.where(adj_matrix[i].toarray() != 0)[1])\n",
        "        return adj_list\n",
        "\n",
        "    def get_features(self):\n",
        "        \"\"\"\n",
        "        When raw features are not available,\n",
        "        build one-hot degree features from the adjacency list.\n",
        "        \"\"\"\n",
        "        max_degree = np.max(np.sum(self.adj_matrix != 0, axis=1))\n",
        "        features = np.zeros((self.adj_matrix.shape[0], max_degree))\n",
        "        for node, neighbors in self.adj_list.items():\n",
        "            features[node, len(neighbors)-1] = 1\n",
        "        return features\n",
        "\n",
        "\n",
        "class Classification(nn.Module):\n",
        "    def __init__(self, emb_size, num_classes):\n",
        "        super(Classification, self).__init__()\n",
        "        self.fc1 = nn.Linear(emb_size, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, embeds):\n",
        "        x = F.elu(self.fc1(embeds))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        logists = torch.log_softmax(x, 1)\n",
        "        return logists\n",
        "\n",
        "\n",
        "class Unsup_Loss(object):\n",
        "    \"\"\"docstring for UnsupervisedLoss\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        super(Unsup_Loss, self).__init__()\n",
        "        self.Q = 10\n",
        "        self.N_WALKS = 4\n",
        "        self.WALK_LEN = 4\n",
        "        self.N_WALK_LEN = 5\n",
        "        self.MARGIN = 3\n",
        "        self.adj_lists = dl.adj_list\n",
        "        self.adj_matrix = dl.adj_matrix\n",
        "        self.train_nodes = dl.nodes_train\n",
        "        self.device = device\n",
        "\n",
        "        self.target_nodes = None\n",
        "        self.positive_pairs = []\n",
        "        self.negative_pairs = []\n",
        "        self.node_positive_pairs = {}\n",
        "        self.node_negative_pairs = {}\n",
        "        self.unique_nodes_batch = []\n",
        "\n",
        "    def get_loss_sage(self, embeddings, nodes):\n",
        "        assert len(embeddings) == len(self.unique_nodes_batch)\n",
        "        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
        "        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
        "\n",
        "        nodes_score = []\n",
        "        print(f'len(self.node_positive_pairs):{ len(self.node_positive_pairs)};len(self.node_negative_pairs):{len(self.node_negative_pairs)} ')\n",
        "        assert len(self.node_positive_pairs) == len(self.node_negative_pairs)\n",
        "        for node in self.node_positive_pairs:\n",
        "            pps = self.node_positive_pairs[node]\n",
        "            nps = self.node_negative_pairs[node]\n",
        "            if len(pps) == 0 or len(nps) == 0:\n",
        "                continue\n",
        "\n",
        "            # Q * Exception(negative score)\n",
        "            indexs = [list(x) for x in zip(*nps)]\n",
        "            node_indexs = [node2index[x] for x in indexs[0]]\n",
        "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
        "            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
        "            neg_score = self.Q*torch.mean(torch.log(torch.sigmoid(-neg_score)), 0)\n",
        "\n",
        "            # multiple positive score\n",
        "            indexs = [list(x) for x in zip(*pps)]\n",
        "            node_indexs = [node2index[x] for x in indexs[0]]\n",
        "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
        "            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
        "            pos_score = torch.log(torch.sigmoid(pos_score))\n",
        "\n",
        "            nodes_score.append(torch.mean(- pos_score - neg_score).view(1,-1))\n",
        "\n",
        "        loss = torch.mean(torch.cat(nodes_score, 0))\n",
        "        return loss\n",
        "\n",
        "    def get_loss_margin(self, embeddings, nodes):\n",
        "        \n",
        "        assert len(embeddings) == len(self.unique_nodes_batch)\n",
        "        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
        "        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
        "\n",
        "        nodes_score = []\n",
        "        \n",
        "        #print(f'len(self.node_positive_pairs): {len(self.node_positive_pairs)}; len(self.node_negative_pairs):{len(self.node_negative_pairs)}')\n",
        "        assert len(self.node_positive_pairs) == len(self.node_negative_pairs)\n",
        "        for node in self.node_positive_pairs:\n",
        "            pps = self.node_positive_pairs[node]\n",
        "            nps = self.node_negative_pairs[node]\n",
        "            if len(pps) == 0 or len(nps) == 0:\n",
        "                continue\n",
        "\n",
        "            indexs = [list(x) for x in zip(*pps)]\n",
        "            node_indexs = [node2index[x] for x in indexs[0]]\n",
        "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
        "            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
        "            pos_score, _ = torch.min(torch.log(torch.sigmoid(pos_score)), 0)\n",
        "\n",
        "            indexs = [list(x) for x in zip(*nps)]\n",
        "            node_indexs = [node2index[x] for x in indexs[0]]\n",
        "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
        "            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
        "            neg_score, _ = torch.max(torch.log(torch.sigmoid(neg_score)), 0)\n",
        "\n",
        "            nodes_score.append(torch.max(torch.tensor(0.0).to(self.device),\n",
        "                                         neg_score-pos_score+self.MARGIN).view(1, -1))\n",
        "        loss = torch.mean(torch.cat(nodes_score, 0), 0)\n",
        "        return loss\n",
        "\n",
        "    def extend_nodes(self, nodes, num_neg=6):\n",
        "        self.positive_pairs = []\n",
        "        self.node_positive_pairs = {}\n",
        "        self.negative_pairs = []\n",
        "        self.node_negative_pairs = {}\n",
        "\n",
        "        self.target_nodes = nodes\n",
        "        self.get_positive_nodes(nodes)\n",
        "        self.get_negative_nodes(nodes, num_neg)\n",
        "        self.unique_nodes_batch = list(set([i for x in self.positive_pairs for i in x])\n",
        "                                       | set([i for x in self.negative_pairs for i in x]))\n",
        "        assert set(self.target_nodes) <= set(self.unique_nodes_batch)\n",
        "        return self.unique_nodes_batch\n",
        "\n",
        "    def get_positive_nodes(self, nodes):\n",
        "        return self._run_random_walks(nodes)\n",
        "\n",
        "    def get_negative_nodes(self, nodes, num_neg):\n",
        "        for node in nodes:\n",
        "            neighbors = set([node])\n",
        "            frontier = set([node])\n",
        "            for _ in range(self.N_WALK_LEN):\n",
        "                current = set()\n",
        "                for outer in frontier:\n",
        "                    current |= self.adj_lists[int(outer)]\n",
        "                frontier = current - neighbors\n",
        "                neighbors |= current\n",
        "            far_nodes = set(self.train_nodes) - neighbors\n",
        "            neg_samples = random.sample(far_nodes, num_neg) if num_neg < len(far_nodes) else far_nodes\n",
        "            self.negative_pairs.extend([(node, neg_node) for neg_node in neg_samples])\n",
        "            self.node_negative_pairs[node] = [(node, neg_node) for neg_node in neg_samples]\n",
        "        return self.negative_pairs\n",
        "\n",
        "    def _run_random_walks(self, nodes):\n",
        "        for node in nodes:\n",
        "            if len(self.adj_lists[int(node)]) == 0:\n",
        "                continue\n",
        "            cur_pairs = []\n",
        "            for _ in range(self.N_WALKS):\n",
        "                curr_node = node\n",
        "                for _ in range(self.WALK_LEN):\n",
        "                    cnts = self.adj_matrix[int(curr_node)].toarray().squeeze()\n",
        "                    neighs = []\n",
        "                    for n in np.where(cnts != 0)[0]:\n",
        "                        neighs.extend([n] * int(cnts[n]))\n",
        "                    # neighs = self.adj_lists[int(curr_node)]\n",
        "                    next_node = random.choice(list(neighs))\n",
        "                    # self co-occurrences are useless\n",
        "                    if next_node != node and next_node in self.train_nodes:\n",
        "                        self.positive_pairs.append((node,next_node))\n",
        "                        cur_pairs.append((node,next_node))\n",
        "                    curr_node = next_node\n",
        "\n",
        "            self.node_positive_pairs[node] = cur_pairs\n",
        "        return self.positive_pairs\n",
        "\n",
        "\n",
        "class SageLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodes a node's using 'convolutional' GraphSage approach\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, out_size, gat=False, gcn=False):\n",
        "        super(SageLayer, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.out_size = out_size\n",
        "\n",
        "        self.gat = gat\n",
        "        self.gcn = gcn\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_size, self.input_size if self.gat or self.gcn else 2 * self.input_size))\n",
        "\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def forward(self, self_feats, aggregate_feats):\n",
        "        \"\"\"\n",
        "        Generates embeddings for a batch of nodes.\n",
        "        nodes\t -- list of nodes\n",
        "        \"\"\"\n",
        "        if self.gat or self.gcn:\n",
        "            combined = aggregate_feats\n",
        "        else:\n",
        "            combined = torch.cat([self_feats, aggregate_feats], dim=1)\n",
        "        combined = F.relu(self.weight.mm(combined.t())).t()\n",
        "        return combined\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"Computes the self-attention between pair of nodes\"\"\"\n",
        "    def __init__(self, input_size, out_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.out_size = out_size\n",
        "        self.attention_raw = nn.Linear(2*input_size, 1, bias=False)\n",
        "        self.attention_emb = nn.Linear(2*out_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, row_embs, col_embs):\n",
        "        if row_embs.size(1) == self.input_size:\n",
        "            att = self.attention_raw\n",
        "        elif row_embs.size(1) == self.out_size:\n",
        "            att = self.attention_emb\n",
        "        e = att(torch.cat((row_embs, col_embs), dim=1))\n",
        "        return F.leaky_relu(e, negative_slope=0.2)\n",
        "\n",
        "class GNN_model(nn.Module):\n",
        "    \"\"\"docstring for GraphSage\"\"\"\n",
        "    def __init__(self, num_layers, out_size, dl, device, gat=False, gcn=False, agg_func='MEAN'):\n",
        "        super(GNN_model, self).__init__()\n",
        "\n",
        "        self.input_size = dl.features.size(1)\n",
        "        self.out_size = out_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gat = gat\n",
        "        self.gcn = gcn\n",
        "        self.device = device\n",
        "        self.agg_func = agg_func\n",
        "\n",
        "        self.raw_features = dl.features\n",
        "        self.adj_lists = dl.adj_list\n",
        "        self.adj_matrix = dl.adj_matrix\n",
        "\n",
        "        for index in range(1, num_layers+1):\n",
        "            layer_size = out_size if index != 1 else self.input_size\n",
        "            setattr(self, 'sage_layer'+str(index), SageLayer(layer_size, out_size, gat=self.gat, gcn=self.gcn))\n",
        "        if self.gat:\n",
        "            self.attention = Attention(self.input_size, out_size)\n",
        "\n",
        "    def forward(self, nodes_batch):\n",
        "        \"\"\"\n",
        "        Generates embeddings for a batch of nodes.\n",
        "        nodes_batch\t-- batch of nodes to learn the embeddings\n",
        "        \"\"\"\n",
        "        lower_layer_nodes = list(nodes_batch)\n",
        "        nodes_batch_layers = [(lower_layer_nodes,)]\n",
        "        for _ in range(self.num_layers):\n",
        "            lower_layer_nodes, lower_samp_neighs, lower_layer_nodes_dict= self._get_unique_neighs_list(lower_layer_nodes)\n",
        "            nodes_batch_layers.insert(0, (lower_layer_nodes, lower_samp_neighs, lower_layer_nodes_dict))\n",
        "\n",
        "        assert len(nodes_batch_layers) == self.num_layers + 1\n",
        "\n",
        "        pre_hidden_embs = self.raw_features\n",
        "        for index in range(1, self.num_layers+1):\n",
        "            nb = nodes_batch_layers[index][0]\n",
        "            pre_neighs = nodes_batch_layers[index-1]\n",
        "            aggregate_feats = self.aggregate(nb, pre_hidden_embs, pre_neighs)\n",
        "            sage_layer = getattr(self, 'sage_layer'+str(index))\n",
        "            if index > 1:\n",
        "                nb = self._nodes_map(nb, pre_neighs)\n",
        "            cur_hidden_embs = sage_layer(self_feats=pre_hidden_embs[nb], aggregate_feats=aggregate_feats)\n",
        "            pre_hidden_embs = cur_hidden_embs\n",
        "\n",
        "        return pre_hidden_embs\n",
        "\n",
        "    def _nodes_map(self, nodes, neighs):\n",
        "        _, samp_neighs, layer_nodes_dict = neighs\n",
        "        assert len(samp_neighs) == len(nodes)\n",
        "        index = [layer_nodes_dict[x] for x in nodes]\n",
        "        return index\n",
        "\n",
        "    def _get_unique_neighs_list(self, nodes, num_sample=10):\n",
        "        _set = set\n",
        "        to_neighs = [self.adj_lists[int(node)] for node in nodes]\n",
        "        if self.gcn or self.gat:\n",
        "            samp_neighs = to_neighs\n",
        "        else:\n",
        "            _sample = random.sample\n",
        "            samp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
        "        samp_neighs = [samp_neigh | set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n",
        "        _unique_nodes_list = list(set.union(*samp_neighs))\n",
        "        i = list(range(len(_unique_nodes_list)))\n",
        "        # unique node 2 index\n",
        "        unique_nodes = dict(list(zip(_unique_nodes_list, i)))\n",
        "        return _unique_nodes_list, samp_neighs, unique_nodes\n",
        "\n",
        "    def aggregate(self, nodes, pre_hidden_embs, pre_neighs):\n",
        "        unique_nodes_list, samp_neighs, unique_nodes = pre_neighs\n",
        "\n",
        "        assert len(nodes) == len(samp_neighs)\n",
        "        indicator = [(nodes[i] in samp_neighs[i]) for i in range(len(samp_neighs))]\n",
        "        assert False not in indicator\n",
        "        if not self.gat and not self.gcn:\n",
        "            samp_neighs = [(samp_neighs[i]-set([nodes[i]])) for i in range(len(samp_neighs))]\n",
        "        if len(pre_hidden_embs) == len(unique_nodes):\n",
        "            embed_matrix = pre_hidden_embs\n",
        "        else:\n",
        "            embed_matrix = pre_hidden_embs[torch.LongTensor(unique_nodes_list)]\n",
        "        # get row and column nonzero indices for the mask tensor\n",
        "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
        "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
        "        # get the edge counts for each edge\n",
        "        edge_counts = self.adj_matrix[nodes][:, unique_nodes_list].toarray()\n",
        "        edge_counts = torch.FloatTensor(edge_counts).to(embed_matrix.device)\n",
        "        torch.sqrt_(edge_counts)\n",
        "        if self.gat:\n",
        "            indices = (torch.LongTensor(row_indices), torch.LongTensor(column_indices))\n",
        "            nodes_indices = torch.LongTensor([unique_nodes[nodes[n]] for n in row_indices])\n",
        "            row_embs = embed_matrix[nodes_indices]\n",
        "            col_embs = embed_matrix[column_indices]\n",
        "            atts = self.attention(row_embs, col_embs).squeeze()\n",
        "            mask = torch.zeros(len(samp_neighs), len(unique_nodes)).to(embed_matrix.device)\n",
        "            mask.index_put_(indices, atts)\n",
        "            mask = mask * edge_counts\n",
        "            # softmax\n",
        "            mask = torch.exp(mask) * (mask != 0).float()\n",
        "            mask = F.normalize(mask, p=1, dim=1)\n",
        "        else:\n",
        "            mask = torch.zeros(len(samp_neighs), len(unique_nodes)).to(embed_matrix.device)\n",
        "            mask[row_indices, column_indices] = 1\n",
        "            # multiply edge counts to mask\n",
        "            mask = mask * edge_counts\n",
        "            mask = F.normalize(mask, p=1, dim=1)\n",
        "            mask = mask.to(embed_matrix.device)\n",
        "\n",
        "        if self.agg_func == 'MEAN':\n",
        "            aggregate_feats = mask.mm(embed_matrix)\n",
        "        elif self.agg_func == 'MAX':\n",
        "            indexs = [x.nonzero() for x in mask != 0]\n",
        "            aggregate_feats = []\n",
        "            for feat in [embed_matrix[x.squeeze()] for x in indexs]:\n",
        "                if len(feat.size()) == 1:\n",
        "                    aggregate_feats.append(feat.view(1, -1))\n",
        "                else:\n",
        "                    aggregate_feats.append(torch.max(feat,0)[0].view(1, -1))\n",
        "            aggregate_feats = torch.cat(aggregate_feats, 0)\n",
        "\n",
        "        return aggregate_feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iODWFyJ9shBP"
      },
      "outputs": [],
      "source": [
        "#@title GaussianMixtureModel code\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.distributions as D\n",
        "\n",
        "class GaussianMixtureModel(torch.nn.Module):\n",
        "    # https://discuss.pytorch.org/t/fit-gaussian-mixture-model/121826\n",
        "\n",
        "    def __init__(self, n_components: int=num_classes):\n",
        "        super().__init__()\n",
        "        weights = torch.ones(n_components, )\n",
        "        means   = torch.randn(n_components, )\n",
        "        stdevs  = torch.tensor(np.abs(np.random.randn(n_components, )))\n",
        "        self.weights = torch.nn.Parameter(weights)\n",
        "        self.means   = torch.nn.Parameter(means)\n",
        "        self.stdevs  = torch.nn.Parameter(stdevs)\n",
        "        \n",
        "    def _update_parameters(self, new_weights, new_means, new_stdevs):\n",
        "        self.weights = torch.nn.Parameter(new_weights)\n",
        "        self.means   = torch.nn.Parameter(new_means)\n",
        "        self.stdevs  = torch.nn.Parameter(new_stdevs)\n",
        "        \n",
        "    def _get_weights(self):\n",
        "        return self.weights\n",
        "    \n",
        "    def _get_parameters(self):\n",
        "        return self.weights, self.means, self.stdevs\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #print(self.weights)\n",
        "        mix  = D.Categorical(self.weights)\n",
        "        #std_weight = 1e-4\n",
        "        #comp = D.Normal(self.means, std_weight * self.stdevs.abs())\n",
        "        comp = D.Normal(self.means, self.stdevs)\n",
        "        gmm  = D.MixtureSameFamily(mix, comp)\n",
        "        return - gmm.log_prob(x).mean()\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1f52vLxGsmgg"
      },
      "outputs": [],
      "source": [
        "#@title Data loader \n",
        "import sys\n",
        "import os\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class DataCenter(object):\n",
        "    \"\"\"docstring for DataCenter\"\"\"\n",
        "    \"\"\"def __init__(self, config):\n",
        "        super(DataCenter, self).__init__()\n",
        "        self.config = config\"\"\"\n",
        "    def __init__(self):\n",
        "        super(DataCenter, self).__init__()\n",
        "        \n",
        "    def load_dataSet(self, dataSet='cora'):\n",
        "        if dataSet == 'cora':\n",
        "            cora_content_file = DATA_DIR + '/cora.content'\n",
        "            cora_cite_file = DATA_DIR + '/cora.cites'\n",
        "\n",
        "            feat_data = []\n",
        "            labels = [] # label sequence of node\n",
        "            node_map = {} # map node to Node_ID\n",
        "            label_map = {} # map label to Label_ID\n",
        "            with open(cora_content_file) as fp:\n",
        "                for i,line in enumerate(fp):\n",
        "                    info = line.strip().split()\n",
        "                    feat_data.append([float(x) for x in info[1:-1]])\n",
        "                    node_map[info[0]] = i\n",
        "                    if not info[-1] in label_map:\n",
        "                        label_map[info[-1]] = len(label_map)\n",
        "                    labels.append(label_map[info[-1]])\n",
        "            feat_data = np.asarray(feat_data)\n",
        "            labels = np.asarray(labels, dtype=np.int64)\n",
        "            \n",
        "            row = []\n",
        "            col = []\n",
        "            adj_lists = defaultdict(set)\n",
        "            with open(cora_cite_file) as fp:\n",
        "                for i,line in enumerate(fp):\n",
        "                    info = line.strip().split()\n",
        "                    assert len(info) == 2\n",
        "                    paper1 = node_map[info[0]]\n",
        "                    paper2 = node_map[info[1]]\n",
        "                    adj_lists[paper1].add(paper2)\n",
        "                    adj_lists[paper2].add(paper1)\n",
        "                    row.extend([paper1, paper2])\n",
        "                    col.extend([paper2, paper1])\n",
        "                    \n",
        "            #row = np.asarray(row)\n",
        "            #col = np.asarray(col)\n",
        "            #adj_matrix = csr_matrix((np.ones(len(row)), (row, col)), shape=(len(node_map), len(node_map)))\n",
        "\n",
        "            assert len(feat_data) == len(labels) == len(adj_lists)\n",
        "            test_indexs, val_indexs, train_indexs = self._split_data(feat_data.shape[0])\n",
        "\n",
        "            setattr(self, dataSet+'_test', test_indexs)\n",
        "            setattr(self, dataSet+'_val', val_indexs)\n",
        "            setattr(self, dataSet+'_train', train_indexs)\n",
        "\n",
        "            setattr(self, dataSet+'_feats', feat_data)\n",
        "            setattr(self, dataSet+'_labels', labels)\n",
        "            setattr(self, dataSet+'_adj_lists', adj_lists)\n",
        "            #setattr(self, dataSet+'_adj_matrix', adj_matrix)\n",
        "\n",
        "        elif dataSet == 'citeseer':\n",
        "          dataset = datasets.Planetoid(root=f'/tmp/Citeseer', name='Citeseer')\n",
        "          data = dataset[0].to(device)\n",
        "\n",
        "          #adj_matrix = to_scipy_sparse_matrix(data.edge_index).tocsr()\n",
        "          feat_data = data.x.to('cpu').detach().numpy().copy()\n",
        "          labels = data.y.to('cpu').detach().numpy().copy()\n",
        "          edge_index = data.edge_index.to('cpu').detach().numpy().copy()\n",
        "\n",
        "          adj_lists = defaultdict(set)\n",
        "          for (origin, dest) in zip(edge_index[0], edge_index[1]):\n",
        "            adj_lists[origin].add(dest)\n",
        "            adj_lists[dest].add(origin)\n",
        "\n",
        "          test_indexs, val_indexs, train_indexs = self._split_data(feat_data.shape[0])\n",
        "\n",
        "          setattr(self, dataSet+'_test', test_indexs)\n",
        "          setattr(self, dataSet+'_val', val_indexs)\n",
        "          setattr(self, dataSet+'_train', train_indexs)\n",
        "\n",
        "          setattr(self, dataSet+'_feats', feat_data)\n",
        "          setattr(self, dataSet+'_labels', labels)\n",
        "          setattr(self, dataSet+'_adj_lists', adj_lists)\n",
        "\n",
        "        elif dataSet == 'cora-full':\n",
        "          dataset = datasets.CoraFull(root=f'/tmp/CoraFull')\n",
        "          data = dataset[0].to(device)\n",
        "\n",
        "          #adj_matrix = to_scipy_sparse_matrix(data.edge_index).tocsr()\n",
        "          feat_data = data.x.to('cpu').detach().numpy().copy()\n",
        "          labels = data.y.to('cpu').detach().numpy().copy()\n",
        "          edge_index = data.edge_index.to('cpu').detach().numpy().copy()\n",
        "\n",
        "          adj_lists = defaultdict(set)\n",
        "          for (origin, dest) in zip(edge_index[0], edge_index[1]):\n",
        "            adj_lists[origin].add(dest)\n",
        "            adj_lists[dest].add(origin)\n",
        "\n",
        "          test_indexs, val_indexs, train_indexs = self._split_data(feat_data.shape[0])\n",
        "\n",
        "          setattr(self, dataSet+'_test', test_indexs)\n",
        "          setattr(self, dataSet+'_val', val_indexs)\n",
        "          setattr(self, dataSet+'_train', train_indexs)\n",
        "\n",
        "          setattr(self, dataSet+'_feats', feat_data)\n",
        "          setattr(self, dataSet+'_labels', labels)\n",
        "          setattr(self, dataSet+'_adj_lists', adj_lists)\n",
        "\n",
        "        elif dataSet == 'coauthor-cs':\n",
        "          dataset = datasets.Coauthor(root=f'/tmp/Coauthor-CS', name='CS')\n",
        "          data = dataset[0].to(device)\n",
        "\n",
        "          #adj_matrix = to_scipy_sparse_matrix(data.edge_index).tocsr()\n",
        "          feat_data = data.x.to('cpu').detach().numpy().copy()\n",
        "          labels = data.y.to('cpu').detach().numpy().copy()\n",
        "          edge_index = data.edge_index.to('cpu').detach().numpy().copy()\n",
        "\n",
        "          adj_lists = defaultdict(set)\n",
        "          for (origin, dest) in zip(edge_index[0], edge_index[1]):\n",
        "            adj_lists[origin].add(dest)\n",
        "            adj_lists[dest].add(origin)\n",
        "\n",
        "          test_indexs, val_indexs, train_indexs = self._split_data(feat_data.shape[0])\n",
        "\n",
        "          setattr(self, dataSet+'_test', test_indexs)\n",
        "          setattr(self, dataSet+'_val', val_indexs)\n",
        "          setattr(self, dataSet+'_train', train_indexs)\n",
        "\n",
        "          setattr(self, dataSet+'_feats', feat_data)\n",
        "          setattr(self, dataSet+'_labels', labels)\n",
        "          setattr(self, dataSet+'_adj_lists', adj_lists)\n",
        "\n",
        "        elif dataSet == 'coauthor-physics':\n",
        "          dataset = datasets.Coauthor(root=f'/tmp/Coauthor-Physics', name='Physics')\n",
        "          data = dataset[0].to(device)\n",
        "\n",
        "          #adj_matrix = to_scipy_sparse_matrix(data.edge_index).tocsr()\n",
        "          feat_data = data.x.to('cpu').detach().numpy().copy()\n",
        "          labels = data.y.to('cpu').detach().numpy().copy()\n",
        "          edge_index = data.edge_index.to('cpu').detach().numpy().copy()\n",
        "\n",
        "          adj_lists = defaultdict(set)\n",
        "          for (origin, dest) in zip(edge_index[0], edge_index[1]):\n",
        "            adj_lists[origin].add(dest)\n",
        "            adj_lists[dest].add(origin)\n",
        "\n",
        "          test_indexs, val_indexs, train_indexs = self._split_data(feat_data.shape[0])\n",
        "\n",
        "          setattr(self, dataSet+'_test', test_indexs)\n",
        "          setattr(self, dataSet+'_val', val_indexs)\n",
        "          setattr(self, dataSet+'_train', train_indexs)\n",
        "\n",
        "          setattr(self, dataSet+'_feats', feat_data)\n",
        "          setattr(self, dataSet+'_labels', labels)\n",
        "          setattr(self, dataSet+'_adj_lists', adj_lists)\n",
        "\n",
        "        elif dataSet == 'pubmed':\n",
        "          dataset = datasets.Planetoid(root=f'/tmp/PubMed', name='PubMed')\n",
        "          data = dataset[0].to(device)\n",
        "\n",
        "          #adj_matrix = to_scipy_sparse_matrix(data.edge_index).tocsr()\n",
        "          feat_data = data.x.to('cpu').detach().numpy().copy()\n",
        "          labels = data.y.to('cpu').detach().numpy().copy()\n",
        "          edge_index = data.edge_index.to('cpu').detach().numpy().copy()\n",
        "\n",
        "          adj_lists = defaultdict(set)\n",
        "          for (origin, dest) in zip(edge_index[0], edge_index[1]):\n",
        "            adj_lists[origin].add(dest)\n",
        "            adj_lists[dest].add(origin)\n",
        "\n",
        "          test_indexs, val_indexs, train_indexs = self._split_data(feat_data.shape[0])\n",
        "\n",
        "          setattr(self, dataSet+'_test', test_indexs)\n",
        "          setattr(self, dataSet+'_val', val_indexs)\n",
        "          setattr(self, dataSet+'_train', train_indexs)\n",
        "\n",
        "          setattr(self, dataSet+'_feats', feat_data)\n",
        "          setattr(self, dataSet+'_labels', labels)\n",
        "          setattr(self, dataSet+'_adj_lists', adj_lists)\n",
        "\n",
        "\n",
        "    def _split_data(self, num_nodes, test_split = 4, val_split = 4):\n",
        "        rand_indices = np.random.permutation(num_nodes)\n",
        "\n",
        "        test_size = num_nodes // test_split\n",
        "        val_size = num_nodes // val_split\n",
        "        train_size = num_nodes - (test_size + val_size)\n",
        "\n",
        "        test_indexs = rand_indices[:test_size]\n",
        "        val_indexs = rand_indices[test_size:(test_size+val_size)]\n",
        "        train_indexs = rand_indices[(test_size+val_size):]\n",
        "        \n",
        "        return test_indexs, val_indexs, train_indexs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n-XSoalzs8W7"
      },
      "outputs": [],
      "source": [
        "#@title Model evluation \n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import math\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(dataCenter, ds, gnn_model, classification_model, device, max_vali_f1, name, cur_epoch):\n",
        "    nodes_test = getattr(dataCenter, ds+'_test')\n",
        "    nodes_val = getattr(dataCenter, ds+'_val')\n",
        "    labels = getattr(dataCenter, ds+'_labels')\n",
        "\n",
        "    models = [gnn_model, classification_model]\n",
        "\n",
        "    params = []\n",
        "    for model in models:\n",
        "        for param in model.parameters():\n",
        "            if param.requires_grad:\n",
        "                param.requires_grad = False\n",
        "                params.append(param)\n",
        "\n",
        "    embs = gnn_model(nodes_val)\n",
        "    logists = classification_model(embs)\n",
        "    _, predicts = torch.max(logists, 1)\n",
        "    labels_val = labels[nodes_val]\n",
        "    assert len(labels_val) == len(predicts)\n",
        "    comps = zip(labels_val, predicts.data)\n",
        "\n",
        "    vali_f1 = f1_score(labels_val, predicts.cpu().data, average=\"micro\")\n",
        "    #print(\"Validation F1:\", vali_f1)\n",
        "\n",
        "    if vali_f1 > max_vali_f1:\n",
        "        max_vali_f1 = vali_f1\n",
        "        embs = gnn_model(nodes_test)\n",
        "        logists = classification_model(embs)\n",
        "        _, predicts = torch.max(logists, 1)\n",
        "        labels_test = labels[nodes_test]\n",
        "        assert len(labels_test) == len(predicts)\n",
        "        comps = zip(labels_test, predicts.data)\n",
        "\n",
        "        test_f1 = f1_score(labels_test, predicts.cpu().data, average=\"micro\")\n",
        "        print(\"Test F1:\", test_f1)\n",
        "\n",
        "        for param in params:\n",
        "            param.requires_grad = True\n",
        "\n",
        "        torch.save(models, 'models/model_best_{}_ep{}_{:.4f}.torch'.format(name, cur_epoch, test_f1))\n",
        "\n",
        "    for param in params:\n",
        "        param.requires_grad = True\n",
        "\n",
        "    return max_vali_f1\n",
        "\n",
        "def get_gnn_embeddings(gnn_model, dataCenter, ds):\n",
        "    print('Loading embeddings from trained GNN model.')\n",
        "    features = np.zeros((len(getattr(dataCenter, ds+'_labels')), gnn_model.out_size))\n",
        "    nodes = np.arange(len(getattr(dataCenter, ds+'_labels'))).tolist()\n",
        "    b_sz = 500\n",
        "    batches = math.ceil(len(nodes) / b_sz)\n",
        "    embs = []\n",
        "    for index in range(batches):\n",
        "        nodes_batch = nodes[index*b_sz:(index+1)*b_sz]\n",
        "        embs_batch = gnn_model(nodes_batch)\n",
        "        assert len(embs_batch) == len(nodes_batch)\n",
        "        embs.append(embs_batch)\n",
        "        # if ((index+1)*b_sz) % 10000 == 0:\n",
        "        #     print(f'Dealed Nodes [{(index+1)*b_sz}/{len(nodes)}]')\n",
        "\n",
        "    assert len(embs) == batches\n",
        "    embs = torch.cat(embs, 0)\n",
        "    assert len(embs) == len(nodes)\n",
        "    print('Embeddings loaded.')\n",
        "    return embs.detach()\n",
        "\n",
        "def train_classification(dataCenter, gnn_model, classification_model, ds, device, max_vali_f1, name, epochs=800):\n",
        "    print('Training Classification ...')\n",
        "    c_optimizer = torch.optim.SGD(classification_model.parameters(), lr=0.5)\n",
        "    # train classification, detached from the current graph\n",
        "    #classification.init_params()\n",
        "    b_sz = 100\n",
        "    train_nodes = getattr(dataCenter, ds+'_train')\n",
        "    labels = getattr(dataCenter, ds+'_labels')\n",
        "    features = get_gnn_embeddings(gnn_model, dataCenter, ds)\n",
        "    for epoch in range(epochs):\n",
        "        train_nodes = shuffle(train_nodes)\n",
        "        batches = math.ceil(len(train_nodes) / b_sz)\n",
        "        visited_nodes = set()\n",
        "        for index in range(batches):\n",
        "            nodes_batch = train_nodes[index*b_sz:(index+1)*b_sz]\n",
        "            visited_nodes |= set(nodes_batch)\n",
        "            labels_batch = labels[nodes_batch]\n",
        "            embs_batch = features[nodes_batch]\n",
        "\n",
        "            logists = classification_model(embs_batch)\n",
        "            loss = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
        "            loss /= len(nodes_batch)\n",
        "            # print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Dealed Nodes [{}/{}] '.format(epoch+1, epochs, index, batches, loss.item(), len(visited_nodes), len(train_nodes)))\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            nn.utils.clip_grad_norm_(classification.parameters(), 5)\n",
        "            c_optimizer.step()\n",
        "            c_optimizer.zero_grad()\n",
        "\n",
        "        max_vali_f1 = evaluate(dataCenter, ds, gnn_model, classification_model, device, max_vali_f1, name, epoch)\n",
        "    return classification_model, max_vali_f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmjaN7TVbo2u"
      },
      "source": [
        "## Section 2: DNML Computation Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoRiAmcZb9Cc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title DNML Computation\n",
        "!pip install pyper\n",
        "\n",
        "\"\"\"DNML GMM\n",
        "\n",
        "Functions to calculate the parametric complexity for DNML.\n",
        "They are only used for comparison methods.\n",
        "\n",
        ".. References:\n",
        "    * Tianyi Wu, Shinya Sugawara, and Kenji Yamanishi. Decposed Normalized\n",
        "    Maximum Likelihood Codelength Criterion for Selecting Hierarchical Latent\n",
        "    Variable Models. In Proceedings of the 23rd ACM SIGKDD International\n",
        "    Conference on Knowledge Discovery and Data Mining. Halifax, Canada,\n",
        "    1165--1174.\n",
        "    * Kenji Yamanishi, Tianyi Wu, Shinya Sugawara, and Makoto Okada.\n",
        "    The Decomposed Normalized Maximum Likelihood Code-Length Criterion for\n",
        "    Selecting Hierarchical Latent Variable Models. Data Mining and Knowledge\n",
        "    Discovery, 33, 1017--1058, 2019.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.special import logsumexp, loggamma\n",
        "\n",
        "\n",
        "def _pc_multinomial(N, K):\n",
        "    \"\"\"parametric complexity for multinomial distributions.\n",
        "\n",
        "    Args:\n",
        "        N (int): number of data.\n",
        "        K (int): number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        float: parametric complexity for multinomial distributions.\n",
        "    \"\"\"\n",
        "    PC_list = [0]\n",
        "\n",
        "    # K = 1\n",
        "    if K >= 1:\n",
        "        PC_list.append(1)\n",
        "\n",
        "    # K = 2\n",
        "    if K >= 2:\n",
        "        r1 = np.arange(N + 1)\n",
        "        r2 = N - r1\n",
        "        logpc_2 = logsumexp(sum([\n",
        "            loggamma(N + 1),\n",
        "            (-1) * loggamma(r1 + 1),\n",
        "            (-1) * loggamma(r2 + 1),\n",
        "            r1 * np.log(r1 / N + 1e-50),\n",
        "            r2 * np.log(r2 / N + 1e-50)\n",
        "        ]))\n",
        "        PC_list.append(np.exp(logpc_2))\n",
        "\n",
        "    # K >= 3\n",
        "    for k in range(3, K + 1):\n",
        "        PC_list.append(\n",
        "            PC_list[k - 1] + N * PC_list[k - 2] / (k - 2)\n",
        "        )\n",
        "\n",
        "    return PC_list[-1]\n",
        "\n",
        "\n",
        "def _log_pc_gaussian(N_list, D, R, lmd_min):\n",
        "    \"\"\"log parametric complexity for a Gaussian distribution.\n",
        "\n",
        "    Args:\n",
        "        N_list (np.ndarray): list of the number of data.\n",
        "        D (int): dimension of data.\n",
        "        R (float): upper bound of ||mean||^2.\n",
        "        lmd_min (float): lower bound of the eigenvalues of the covariance\n",
        "            matrix.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: list of the parametric complexity.\n",
        "    \"\"\"\n",
        "    N_list = np.array(N_list)\n",
        "\n",
        "    log_PC_list = sum([\n",
        "        D * N_list * np.log(N_list / 2 / math.e) / 2,\n",
        "        (-1) * D * (D - 1) * np.log(math.pi) / 4,\n",
        "        (-1) * np.sum(\n",
        "            loggamma((N_list.reshape(-1, 1) - np.arange(1, D + 1)) / 2),\n",
        "            axis=1\n",
        "        ),\n",
        "        (D + 1) * np.log(2 / D),\n",
        "        (-1) * loggamma(D / 2),\n",
        "        D * np.log(R) / 2,\n",
        "        (-1) * D**2 * np.log(lmd_min) / 2\n",
        "    ])\n",
        "\n",
        "    return log_PC_list\n",
        "\n",
        "\n",
        "def log_pc_gmm(K_max, N_max, D, *, R=1e+3, lmd_min=1e-3):\n",
        "    \"\"\"log PC of GMM.\n",
        "\n",
        "    Calculate (log) parametric complexity of Gaussian mixture model.\n",
        "\n",
        "    Args:\n",
        "        K_max (int): max number of clusters.\n",
        "        N_max (int): max number of data.\n",
        "        D (int): dimension of data.\n",
        "        R (float): upper bound of ||mean||^2.\n",
        "        lmd_min (float): lower bound of the eigenvalues of the covariance\n",
        "            matrix.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: array of (log) parametric complexity.\n",
        "            returns[K, N] = log C(K, N)\n",
        "    \"\"\"\n",
        "    log_PC_array = np.zeros([K_max + 1, N_max + 1])\n",
        "    r1_min = D + 1\n",
        "\n",
        "    # N = 0\n",
        "    log_PC_array[:, 0] = -np.inf\n",
        "\n",
        "    # K = 0\n",
        "    log_PC_array[0, :] = -np.inf\n",
        "\n",
        "    # K = 1\n",
        "    # N <= r1_min\n",
        "    log_PC_array[1, :r1_min] = -np.inf\n",
        "    # N > r1_min\n",
        "    N_list = np.arange(r1_min, N_max + 1)\n",
        "    log_PC_array[1, r1_min:] = _log_pc_gaussian(\n",
        "        N_list,\n",
        "        D=D,\n",
        "        R=R,\n",
        "        lmd_min=lmd_min\n",
        "    )\n",
        "\n",
        "    # K > 1\n",
        "    for k in range(2, K_max + 1):\n",
        "        for n in range(1, N_max + 1):\n",
        "            r1 = np.arange(n + 1)\n",
        "            r2 = n - r1\n",
        "            log_PC_array[k, n] = logsumexp(sum([\n",
        "                loggamma(n + 1),\n",
        "                (-1) * loggamma(r1 + 1),\n",
        "                (-1) * loggamma(r2 + 1),\n",
        "                r1 * np.log(r1 / n + 1e-100),\n",
        "                r2 * np.log(r2 / n + 1e-100),\n",
        "                log_PC_array[1, r1],\n",
        "                log_PC_array[k - 1, r2]\n",
        "            ]))\n",
        "\n",
        "    return log_PC_array\n",
        "\n",
        "\n",
        "from copy import deepcopy\n",
        "import sys\n",
        "\n",
        "sys.path.append('../')\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "\"\"\"Functions for Gaussian Mixture Model.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import RandomState\n",
        "from scipy.special import logsumexp\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
        "\n",
        "\n",
        "\n",
        "class GMMUtils():\n",
        "    \"\"\"Useful Functions for Gaussian Mixture Model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rho, means, covariances):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            rho (ndarray): Mixture proportion (shape = (K,)).\n",
        "            means (ndarray): Mean vectors (shape = (K, D)).\n",
        "            covariances (ndarray): Covariance matrices (shape = (K, D, D)).\n",
        "        \"\"\"\n",
        "        self.rho = rho\n",
        "        self.means = means\n",
        "        self.covariances = covariances\n",
        "        self.K = len(rho)\n",
        "\n",
        "    def sample(self, N=100, random_state=None):\n",
        "        \"\"\"Sample from GMM.\n",
        "\n",
        "        Args:\n",
        "            N (int): Number of the points to sample.\n",
        "            random_state (Optional[int]): Random state.\n",
        "        Returns:\n",
        "            ndarray: Sampled points (shape = (N, K)).\n",
        "        \"\"\"\n",
        "        random = RandomState(random_state)\n",
        "        nk = random.multinomial(N, self.rho)\n",
        "        X = []\n",
        "        for mean, cov, size in zip(self.means, self.covariances, nk):\n",
        "            X_new = multivariate_normal.rvs(\n",
        "                mean=mean,\n",
        "                cov=cov,\n",
        "                size=size,\n",
        "                random_state=random\n",
        "            )\n",
        "            if size == 0:\n",
        "                pass\n",
        "            elif size == 1:\n",
        "                X.append(X_new)\n",
        "            else:\n",
        "                X.extend(X_new)\n",
        "        return np.array(X)\n",
        "\n",
        "    def logpdf(self, X):\n",
        "        \"\"\"Calculate log pdf.\n",
        "\n",
        "        Args:\n",
        "            X (ndarray): Data (shape = (N, D)).\n",
        "        Returns:\n",
        "            ndarray: Matrix of log pdf (shape = (N, K)).\n",
        "        \"\"\"\n",
        "        N = len(X)\n",
        "        log_pdf = np.zeros([N, self.K])\n",
        "        for k in range(self.K):\n",
        "            log_pdf[:, k] = multivariate_normal.logpdf(\n",
        "                X,\n",
        "                self.means[k],\n",
        "                self.covariances[k],\n",
        "                allow_singular=True\n",
        "            )\n",
        "        return log_pdf\n",
        "\n",
        "    def prob_latent(self, X):\n",
        "        \"\"\"Probability of the latent variables.\n",
        "\n",
        "        Args:\n",
        "            X (ndarray): Data (shape = (N, D)).\n",
        "        Returns:\n",
        "            ndarray: Matrix of latent probabilities (shape = (N, K)).\n",
        "        \"\"\"\n",
        "        log_pdf = self.logpdf(X)\n",
        "        log_rho_pdf = np.log(self.rho + 1e-50) + log_pdf\n",
        "        log_prob = (\n",
        "            log_rho_pdf -\n",
        "            logsumexp(log_rho_pdf, axis=1).reshape((-1, 1))\n",
        "        )\n",
        "        return np.exp(log_prob)\n",
        "\n",
        "\n",
        "def _comp_loglike(*, X, Z, rho, means, covariances):\n",
        "    \"\"\"complete log-likelihood\n",
        "\n",
        "    Args:\n",
        "        X (ndarray): Data (shape = (N, K)).\n",
        "        Z (ndarray): Latent variables (shape = (N,)).\n",
        "        rho (ndarray): Mixture proportion (shape = (K,)).\n",
        "        means (ndarray): Mean vectors (shape = (K, D)).\n",
        "        covariances (ndarray): Covariance matrices (shape = (K, D, D)).\n",
        "    Returns:\n",
        "        float: Complete log likelihood.\n",
        "    \"\"\"\n",
        "    _, D = X.shape\n",
        "    K = len(means)\n",
        "    nk = np.bincount(Z, minlength=K)\n",
        "\n",
        "    if min(nk) <= 0:\n",
        "        return np.nan\n",
        "    else:\n",
        "        c_loglike = 0\n",
        "        for k in range(K):\n",
        "            #print(f'current c_loglike = {c_loglike} with k = {k} out of {range(K)}...')\n",
        "            c_loglike += nk[k] * np.log(rho[k])\n",
        "            #print(f'nk[k]={nk[k]}; rho[k]={rho[k]}; c_loglike={c_loglike}')\n",
        "            c_loglike -= 0.5 * nk[k] * D * np.log(2 * math.pi * math.e)\n",
        "            #print(f'np.log(2 * math.pi * math.e)={np.log(2 * math.pi * math.e)}; c_loglike={c_loglike}')\n",
        "            c_loglike -= 0.5 * nk[k] * np.log(np.linalg.det(covariances[k]))\n",
        "            #print(f'covariances[k]={covariances[k]};np.log(np.linalg.det(covariances[k])={np.log(np.linalg.det(covariances[k]))}; c_loglike={c_loglike}')\n",
        "        return c_loglike\n",
        "\n",
        "\n",
        "class GMMModelSelection():\n",
        "    \"\"\"Model Selection of Gaussian Mixture Model.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        K_min=1,\n",
        "        K_max=20,\n",
        "        reg_covar=1e-3,\n",
        "        random_state=None,\n",
        "        mode='GMM_BIC',\n",
        "        weight_concentration_prior=1.0,\n",
        "        tol=1e-3,\n",
        "        degrees_of_freedom_prior=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            K_max (int): Maximum number of the components.\n",
        "            reg_covar (float): Reguralization for covariance.\n",
        "            random_state (Optional[int]): Random state.\n",
        "            mode (str): Estimation mode. Choose from the following:\n",
        "                'GMM_BIC' (EM algorithm + BIC)\n",
        "                'GMM_DNML' (EM algorithm + DNML)\n",
        "                'BGMM' (Variational Bayes based on Dirichlet distribution).\n",
        "            weight_concentration_prior (float): Weight concentration prior\n",
        "                for BGMM.\n",
        "            tol (float): Tolerance for GMM convergence.\n",
        "        \"\"\"\n",
        "        self.K_max = K_max\n",
        "        self.K_min = K_min\n",
        "        self.reg_covar = reg_covar\n",
        "        self.random_state = random_state\n",
        "        self.mode = mode\n",
        "        self.weight_concentration_prior = weight_concentration_prior\n",
        "        self.tol = tol\n",
        "        self.degrees_of_freedom_prior = degrees_of_freedom_prior\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"Select the best model.\n",
        "\n",
        "        Args:\n",
        "            X (ndarray): Data (shape = (N, K)).\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.mode == 'GMM_DNML':\n",
        "            log_pc_array = log_pc_gmm(\n",
        "                K_max=self.K_max,\n",
        "                N_max=X.shape[0],\n",
        "                D=X.shape[1]\n",
        "            )\n",
        "\n",
        "        if self.mode in ['GMM_BIC', 'GMM_DNML']:\n",
        "            model_list = []\n",
        "            criterion_list = []\n",
        "            for K in range(self.K_min, self.K_max + 1):\n",
        "                #print(f'Fitting a GMM with K={K} components...')\n",
        "                # Fit\n",
        "                model_new = GaussianMixture(\n",
        "                    n_components=K,\n",
        "                    reg_covar=self.reg_covar,\n",
        "                    random_state=self.random_state,\n",
        "                    n_init=10,\n",
        "                    tol=self.tol,\n",
        "                    max_iter=10000\n",
        "                )\n",
        "                model_new.fit(X)\n",
        "                model_list.append(model_new)\n",
        "                # Calculate information criterion\n",
        "                if self.mode == 'GMM_BIC':\n",
        "                    criterion_list.append(model_new.bic(X))\n",
        "                elif self.mode == 'GMM_DNML':\n",
        "                    Z = model_new.predict(X)\n",
        "                    loglike = _comp_loglike(\n",
        "                        X=X,\n",
        "                        Z=Z,\n",
        "                        rho=model_new.weights_,\n",
        "                        means=model_new.means_,\n",
        "                        covariances=model_new.covariances_\n",
        "                    )\n",
        "                    complexity = np.log(_pc_multinomial(len(X), K))\n",
        "                    for k in range(K):\n",
        "                        Z_k = sum(Z == k)\n",
        "                        if log_pc_array[1, Z_k] != - np.inf:\n",
        "                            complexity += log_pc_array[1, Z_k]\n",
        "                    criterion_list.append(- loglike + complexity)\n",
        "                    #print(f'loglike:{loglike}, complexity:{complexity}')\n",
        "                    #print(criterion_list)\n",
        "            idx_best = np.nanargmin(criterion_list)\n",
        "            self.model_best_ = model_list[idx_best]\n",
        "\n",
        "        elif self.mode == 'BGMM':\n",
        "            self.model_best_ = BayesianGaussianMixture(\n",
        "                n_components=self.K_max,\n",
        "                reg_covar=self.reg_covar,\n",
        "                random_state=self.random_state,\n",
        "                weight_concentration_prior=self.weight_concentration_prior,\n",
        "                weight_concentration_prior_type='dirichlet_distribution',\n",
        "                max_iter=10000,\n",
        "                n_init=10,\n",
        "                tol=self.tol,\n",
        "                degrees_of_freedom_prior=self.degrees_of_freedom_prior\n",
        "            )\n",
        "            self.model_best_.fit(X)\n",
        "        else:\n",
        "            raise ValueError('methods should be GMM_BIC, GMM_DNML or BGMM.')\n",
        "\n",
        "        self.K_ = self.model_best_.n_components\n",
        "        self.rho_ = self.model_best_.weights_\n",
        "        self.means_ = self.model_best_.means_\n",
        "        self.covariances_ = self.model_best_.covariances_\n",
        "        return(criterion_list)\n",
        "\n",
        "    def prob_latent(self, X):\n",
        "        \"\"\"Probability of the latent variables.\n",
        "\n",
        "        Args:\n",
        "            X (ndarray): Data (shape = (N, D)).\n",
        "        Returns:\n",
        "            ndarray: Matrix of latent probabilities (shape = (N, K)).\n",
        "        \"\"\"\n",
        "        analysis = GMMUtils(\n",
        "            rho=self.rho_,\n",
        "            means=self.means_,\n",
        "            covariances=self.covariances_\n",
        "        )\n",
        "        return analysis.prob_latent(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict latent labels.\n",
        "\n",
        "        Args:\n",
        "            X (ndarray): Data (shape = (N, D)).\n",
        "        Returns:\n",
        "            ndarray: predicted labels (shape = (N,)).\n",
        "        \"\"\"\n",
        "        prob_latent_ = self.prob_latent(X)\n",
        "        return np.argmax(prob_latent_, axis=1)\n",
        "\n",
        "    \n",
        "def experiment_gmm_dnml(X, Z_true, repeat=1):\n",
        "    \n",
        "    X = X\n",
        "    Z_true = Z_true\n",
        "    \n",
        "    def one_trial(seed):\n",
        "        gmm = GMMModelSelection(K_min=num_classes, K_max=num_classes, mode='GMM_DNML', random_state=seed, tol=1e-3)\n",
        "        l_ = gmm.fit(X)\n",
        "        #Z_pred = gmm.predict(X)\n",
        "        K_ = gmm.K_\n",
        "        #f_measure_ = f_measure(Z_true, Z_pred)\n",
        "        #ari_ = ari(Z_true, Z_pred)\n",
        "\n",
        "        return K_, l_ # f_measure_, ari_,\n",
        "\n",
        "    K_list = []\n",
        "    #f_measure_list = []\n",
        "    #ari_list = []\n",
        "    l_list = []\n",
        "\n",
        "    for t in tqdm(range(repeat), leave=False):\n",
        "        K_, l_ = one_trial(seed=t) #f_measure_, ari_,\n",
        "        #K_list.append(K_)\n",
        "        #f_measure_list.append(f_measure_)\n",
        "        #ari_list.append(ari_)\n",
        "        l_list.append(l_)\n",
        "\n",
        "    #print('--- average score ---')\n",
        "    #print(f'  K:         {np.mean(K_list)}')\n",
        "    #print(f'  DNML:         {np.mean(l_list)}')\n",
        "    #print(f'  f_measure: {np.mean(f_measure_list)}')\n",
        "    #print(f'  ARI:       {np.mean(ari_list)}')\n",
        "    return l_list #K_list, f_measure_list, ari_list, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Npop933RFfMD"
      },
      "outputs": [],
      "source": [
        "#@title Conditional Reversion \n",
        "!pip install dgl\n",
        "\n",
        "class InnerProductDecoder(torch.nn.Module):\n",
        "    r\"\"\"The inner product decoder from the `\"Variational Graph Auto-Encoders\"\n",
        "    <https://arxiv.org/abs/1611.07308>`_ paper\n",
        "\n",
        "    .. math::\n",
        "        \\sigma(\\mathbf{Z}\\mathbf{Z}^{\\top})\n",
        "\n",
        "    where :math:`\\mathbf{Z} \\in \\mathbb{R}^{N \\times d}` denotes the latent\n",
        "    space produced by the encoder.\"\"\"\n",
        "    def forward(self, z, edge_index, sigmoid=True):\n",
        "        r\"\"\"Decodes the latent variables :obj:`z` into edge probabilities for\n",
        "        the given node-pairs :obj:`edge_index`.\n",
        "\n",
        "        Args:\n",
        "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
        "            sigmoid (bool, optional): If set to :obj:`False`, does not apply\n",
        "                the logistic sigmoid function to the output.\n",
        "                (default: :obj:`True`)\n",
        "        \"\"\"\n",
        "        value = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1) \n",
        "        return torch.sigmoid(value) if sigmoid else value\n",
        "\n",
        "\n",
        "    def forward_all(self, z, sigmoid=True):\n",
        "        r\"\"\"Decodes the latent variables :obj:`z` into a probabilistic dense\n",
        "        adjacency matrix.\n",
        "\n",
        "        Args:\n",
        "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
        "            sigmoid (bool, optional): If set to :obj:`False`, does not apply\n",
        "                the logistic sigmoid function to the output.\n",
        "                (default: :obj:`True`)\n",
        "        \"\"\"\n",
        "        adj = torch.matmul(z, z.t())# F.cosine_similarity(z, z) #torch.matmul(z, z.t()) # F.cosine_similarity\n",
        "        return torch.sigmoid(adj) if sigmoid else adj\n",
        "    \n",
        "\n",
        "class CosineSimilarityDecoder(torch.nn.Module):\n",
        "    r\"\"\"The inner product decoder from the `\"Variational Graph Auto-Encoders\"\n",
        "    <https://arxiv.org/abs/1611.07308>`_ paper\n",
        "\n",
        "    .. math::\n",
        "        \\sigma(\\mathbf{Z}\\mathbf{Z}^{\\top})\n",
        "\n",
        "    where :math:`\\mathbf{Z} \\in \\mathbb{R}^{N \\times d}` denotes the latent\n",
        "    space produced by the encoder.\"\"\"\n",
        "\n",
        "\n",
        "    def forward_all(self, z, sigmoid=True):\n",
        "        r\"\"\"Decodes the latent variables :obj:`z` into a probabilistic dense\n",
        "        adjacency matrix.\n",
        "\n",
        "        Args:\n",
        "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
        "            sigmoid (bool, optional): If set to :obj:`False`, does not apply\n",
        "                the logistic sigmoid function to the output.\n",
        "                (default: :obj:`True`)\n",
        "        \"\"\"\n",
        "        #adj = torch.matmul(z, z.t())# F.cosine_similarity(z, z) #torch.matmul(z, z.t()) # F.cosine_similarity\n",
        "        adj = torch.empty(0,z.shape[0])\n",
        "        for n in range(z.shape[0]):\n",
        "            if n % 1000 == 0: \n",
        "                print(f'decoded {n} nodes')\n",
        "            score = F.cosine_similarity(z[n], z) \n",
        "            adj = torch.cat([adj, score[None, :]], axis=0)\n",
        "        return torch.sigmoid(adj) if sigmoid else adj\n",
        "    \n",
        "\n",
        "def th_delete(tensor, indices):\n",
        "    mask = th.ones(tensor.numel(), dtype=th.bool)\n",
        "    mask[indices] = False\n",
        "    return tensor[mask]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGAvRu-Uq5s2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Embeddings Selection\n",
        "import pickle\n",
        "from itertools import chain\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# conduct augmented embeddings selection\n",
        "class EmbeddingsSelection():\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      emb,\n",
        "      num_classes,\n",
        "      true_labels,\n",
        "      prop_B=0.1, \n",
        "      num_candidates=500,\n",
        "\n",
        "      ):\n",
        "    self.prop_B = prop_B\n",
        "    self.emb = emb\n",
        "    self.num_classes = num_classes\n",
        "    self.B = math.ceil(self.prop_B*self.emb.shape[0])\n",
        "    self.num_candidates = num_candidates\n",
        "    self.true_labels = true_labels\n",
        "\n",
        "    # create placeholders\n",
        "    self.lst_emb_samples = []\n",
        "    self.lst_label_samples = []\n",
        "    self.lst_augmented_emb_dnml = []\n",
        "\n",
        "\n",
        "    self.pretrained_gmm = GaussianMixture(n_components=self.num_classes, random_state=42).fit(self.emb)\n",
        "    self.gmm_labels = self.pretrained_gmm.predict(self.emb)\n",
        "\n",
        "    # mapping between GMM cluster label and true class label through majority vote\n",
        "    self.df_mapping = pd.DataFrame()\n",
        "    self.df_mapping[\"gmm_labels\"] = self.gmm_labels\n",
        "    self.df_mapping[\"true_labels\"] = self.true_labels\n",
        "\n",
        "    self.df_mapping = self.df_mapping.groupby(['gmm_labels', 'true_labels']).size()\n",
        "    self.df_mapping = pd.DataFrame(self.df_mapping).reset_index()\n",
        "    self.df_mapping = self.df_mapping.loc[self.df_mapping.reset_index().groupby(['true_labels'])[0].idxmax()]\n",
        "\n",
        "    self.gmm_lbl = np.array(self.df_mapping.gmm_labels)  \n",
        "    self.true_lbl = np.array(self.df_mapping.true_labels)\n",
        "\n",
        "\n",
        "  # Getting GMM predicted label accuracy for all data\n",
        "  def _get_GMM_label_accuracy(self):\n",
        "    df_gmm_acc = pd.DataFrame()\n",
        "    df_gmm_acc[\"gmm_labels\"] = self.gmm_labels\n",
        "    df_gmm_acc[\"true_labels\"] = self.true_labels\n",
        "\n",
        "    #GMM label : true class label \n",
        "    label_map_dict = {self.gmm_lbl[i]: self.true_lbl[i] for i in range(len(self.true_lbl))}\n",
        "\n",
        "    # map GMM labels to its corresponding true class label \n",
        "    df_gmm_acc = df_gmm_acc.replace({\"gmm_labels\": label_map_dict})\n",
        "\n",
        "    print(f\"Accuracy of GMM predicted labels is: {len(df_gmm_acc[df_gmm_acc.gmm_labels==df_gmm_acc.true_labels])/len(df_gmm_acc)}\")\n",
        "\n",
        "\n",
        "  def compute_dnml(self):\n",
        "    all_samples = self.pretrained_gmm.sample(n_samples=self.num_candidates * self.B)\n",
        "\n",
        "    sample_df = pd.DataFrame(all_samples[0])\n",
        "    sample_df['label'] = all_samples[1]\n",
        "\n",
        "    for _ in range(self.num_candidates): \n",
        "\n",
        "        sub_sample_df = sample_df.sample(self.B, replace=False)\n",
        "\n",
        "        emb_sample = sub_sample_df.iloc[: , :-1]\n",
        "        label_sample = sub_sample_df.iloc[: , -1]\n",
        "\n",
        "        out = np.zeros_like(label_sample)\n",
        "        for gl,tl in zip(self.gmm_lbl,self.true_lbl):\n",
        "            out[label_sample==gl] = tl\n",
        "        label_sample = out\n",
        "\n",
        "        #print(f'emb_sample:{emb_sample.shape}; label_sample:{label_sample.shape}, sub_sample_df: {sub_sample_df.shape}')\n",
        "        self.lst_emb_samples.append(emb_sample)\n",
        "        self.lst_label_samples.append(label_sample)\n",
        "\n",
        "\n",
        "        dnml_list = experiment_gmm_dnml(X=emb_sample, Z_true=label_sample)\n",
        "        self.lst_augmented_emb_dnml.append(dnml_list)\n",
        "\n",
        "        if _ % 20 == 0:\n",
        "          print(f\"Finish {_}th candidate.\")\n",
        "    \n",
        "    # placeholders    \n",
        "    aug_full_emb_list = []\n",
        "    aug_full_lbl_list = []\n",
        "    for i in range(self.num_candidates):\n",
        "        aug_full_lbl_list.append(np.concatenate([self.true_labels, self.lst_label_samples[i]]))\n",
        "        aug_full_emb_list.append(np.concatenate([self.emb, self.lst_emb_samples[i]]))\n",
        "\n",
        "    lst_aug_emb_dnml_mean = np.mean(self.lst_augmented_emb_dnml, axis=1)\n",
        "    lst_aug_emb_dnml_mean = list(chain(*lst_aug_emb_dnml_mean))\n",
        "\n",
        "    # get best graph embeddings' index\n",
        "    self.dnml_min_id = np.argmin(lst_aug_emb_dnml_mean)\n",
        "\n",
        "    # best performer min\n",
        "    self.dnml_min_emb = aug_full_emb_list[self.dnml_min_id]\n",
        "    self.dnml_min_lbl = aug_full_lbl_list[self.dnml_min_id]\n",
        "    self.dnml_min_lbl = torch.tensor(self.dnml_min_lbl).type(torch.LongTensor)\n",
        "\n",
        "    del aug_full_lbl_list, aug_full_emb_list\n",
        "\n",
        "    # return the selected full augmented embeddings, and its labels \n",
        "    return(self.dnml_min_emb, self.dnml_min_lbl, self.B)\n",
        "\n",
        "    \"\"\"with open(EXP_RESULT_DIR + f'/{ds}_aug_emb_dnml_mean_C={n_candidates}_prop_B={prop_B}', 'wb') as fp:\n",
        "        pickle.dump(lst_aug_emb_dnml_mean, fp)\"\"\"\n",
        "\n",
        "  def _get_graph_info(self):\n",
        "    return(self.dnml_min_id, self.dnml_min_emb, self.dnml_min_lbl, self.df_mapping)\n",
        "\n",
        "  def _plot_tsne(self):\n",
        "    print(\"Plotting TSEN project of the best augmented embeddings...\")\n",
        "\n",
        "    tsne = TSNE(n_components=2, verbose=1, random_state=123)\n",
        "    emb_tsne = tsne.fit_transform(self.dnml_min_emb) \n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    #df[\"y\"] = self.true_labels\n",
        "    df[\"comp-1\"] = emb_tsne[:,0]\n",
        "    df[\"comp-2\"] = emb_tsne[:,1]\n",
        "    df[\"gmm_labels\"] = self.dnml_min_lbl\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    ax = sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.gmm_labels.tolist(),\n",
        "                    palette=sns.color_palette(\"hls\", k_hat),\n",
        "                    data=df)\n",
        "\n",
        "    ax.get_legend().remove()\n",
        "    ax.set(title=f\"Selected Embeddings GMM Labels \\n ($C\\%={self.num_candidates}$, {base_model_name})\")\n",
        "    fig = ax.get_figure()\n",
        "    return fig\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVv5rK6BcFu3"
      },
      "source": [
        "## Section 3: Downstream Node Classificaiton Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5MgwT4xb9G1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "#os.environ['TORCH'] = torch.__version__\n",
        "os.environ['TORCH'] = '1.13.0+cu116' # now the system is 1.13.1+cu116, but no precompiled wheel of it\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "\n",
        "#@title Define downstream GCN, GAT, GraphSAGE  \n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv, GCNConv, SAGEConv\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self,num_node_features, num_classes, emb_dim=16):\n",
        "        super(GAT, self).__init__()\n",
        "        self.hid = emb_dim\n",
        "        self.in_head = 8\n",
        "        self.out_head = 1\n",
        "        \n",
        "        \n",
        "        self.conv1 = GATConv(num_node_features, self.hid, heads=self.in_head, dropout=0.6)\n",
        "        self.conv2 = GATConv(self.hid*self.in_head, num_classes, concat=False,\n",
        "                             heads=self.out_head, dropout=0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "                \n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes, emb_dim=128):\n",
        "        super().__init__()\n",
        "        self.hid = emb_dim\n",
        "        self.conv1 = GCNConv(num_node_features, self.hid)\n",
        "        self.conv2 = GCNConv(self.hid, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "  \"\"\"GraphSAGE\"\"\"\n",
        "  def __init__(self, num_node_features, num_classes, emb_dim=8):\n",
        "    super().__init__()\n",
        "    self.hid = emb_dim\n",
        "    self.sage1 = SAGEConv(num_node_features, self.hid)\n",
        "    self.sage2 = SAGEConv(self.hid, num_classes)\n",
        "\n",
        "  def forward(self, data):\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "\n",
        "    h = self.sage1(x, edge_index).relu()\n",
        "    h = F.dropout(h, p=0.5, training=self.training)\n",
        "    h = self.sage2(h, edge_index)\n",
        "    return F.log_softmax(h, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTgFoOrVyySb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Node Classification\n",
        "from torch_geometric import datasets\n",
        "from torch_geometric.utils.convert import to_scipy_sparse_matrix\n",
        "# Implementation of matplotlib spy function\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "class NodeClassification():\n",
        "  \"\"\"Downstream Node Classification\"\"\"\n",
        "  def __init__(\n",
        "        self,\n",
        "        B,\n",
        "        emb,\n",
        "        ds,\n",
        "        gmm_lbl,\n",
        "        generate_label=False,\n",
        "        threasold=0.75\n",
        "    ):\n",
        "    self.emb = emb\n",
        "    self.gmm_lbl = gmm_lbl.to(device)\n",
        "    self.B = B\n",
        "    self.decoder = CosineSimilarityDecoder()\n",
        "    if ds in ['cora-full']: #cora-full\n",
        "      #self.org_dataset = datasets.CoraFull(root=f'/tmp/{ds}')\n",
        "      with open(f\"/content/drive/MyDrive/gda_dnml/{ds}/data/{ds}-preprocessed-data\", 'rb') as fp:\n",
        "          self.org_data = pickle.load(fp)\n",
        "      with open(f\"/content/drive/MyDrive/gda_dnml/{ds}/data/{ds}-preprocessed-dataset\", 'rb') as fp:\n",
        "          self.org_dataset = pickle.load(fp)\n",
        "    elif ds in ['Cora', 'PubMed', 'Citeseer']:\n",
        "      self.org_dataset = datasets.Planetoid(root=f'/tmp/{ds}', name=ds)\n",
        "    elif ds in ['coauthor-cs']:\n",
        "      #self.org_dataset = datasets.Coauthor(root=f'/tmp/{ds}', name='CS')\n",
        "      with open(f\"/content/drive/MyDrive/gda_dnml/{ds}/data/{ds}-preprocessed-data\", 'rb') as fp:\n",
        "          self.org_data = pickle.load(fp)\n",
        "\n",
        "      with open(f\"/content/drive/MyDrive/gda_dnml/{ds}/data/{ds}-preprocessed-dataset\", 'rb') as fp:\n",
        "          self.org_dataset = pickle.load(fp)\n",
        "\n",
        "    elif ds in ['coauthor-physics']:\n",
        "      #self.org_dataset = datasets.Coauthor(root=f'/tmp/{ds}', name='Physics')\n",
        "      with open(f\"/content/drive/MyDrive/gda_dnml/{ds}/data/{ds}-preprocessed-data\", 'rb') as fp:\n",
        "          self.org_data = pickle.load(fp)\n",
        "\n",
        "      with open(f\"/content/drive/MyDrive/gda_dnml/{ds}/data/{ds}-preprocessed-dataset\", 'rb') as fp:\n",
        "          self.org_dataset = pickle.load(fp)\n",
        "\n",
        "\n",
        "    self.org_data = self.org_data.to(device)\n",
        "    self.org_data.edge_index = self.org_data.edge_index.to(device)\n",
        "    self.org_adj = to_scipy_sparse_matrix(self.org_data.edge_index)\n",
        "\n",
        "    self.num_classes = self.org_dataset.num_classes\n",
        "\n",
        "    # decode and revert back to adjacency matrix \n",
        "    self.decode_embeddings(self.emb, threasold)\n",
        "\n",
        "\n",
        "\n",
        "  def decode_embeddings(self, emb, threasold):\n",
        "    self.emb = torch.tensor(emb)\n",
        "\n",
        "    self.aug_g = self.decoder.forward_all(self.emb)\n",
        "    self.aug_g = self.aug_g.fill_diagonal_(0)   \n",
        "\n",
        "    self.aug_g = (self.aug_g>=threasold).float()\n",
        "    #print(f'shape of  self.aug_g is{ self.aug_g.shape}')\n",
        "\n",
        "    val = torch.from_numpy(self.org_adj.data.astype(np.float64)).to(torch.float).to(device)    #Presuming values are floats, can use np.int64 for dtype=int8\n",
        "    out = torch.sparse.FloatTensor(self.org_data.edge_index, val, torch.Size(self.org_adj.shape)).to_dense() \n",
        "\n",
        "    self.aug_g[:self.org_adj.shape[0],:self.org_adj.shape[1]] = out\n",
        "    #print(f'shape of  self.aug_g is{ self.aug_g.shape}')\n",
        "\n",
        "\n",
        "  def _plot_adjacencies(self, markersize=0.5):\n",
        "    figure, axis = plt.subplots(1, 2, figsize=(15, 15))\n",
        "      \n",
        "    # plot original adjacency matrix\n",
        "    axis[0].spy(self.org_adj, markersize = markersize)\n",
        "    axis[0].set_title(\"Adjacency Plot of Original Matrix\")\n",
        "\n",
        "    # plot conditionally reverted augmented adjacency matrix\n",
        "    axis[1].spy(self.aug_g, markersize = markersize)\n",
        "    axis[1].set_title(\"Adjacency Plot of Conditionally Reverted Augmented Matrix\")\n",
        "\n",
        "  def _data_prepare(self, generate_label):  \n",
        "    self.aug_data = self.org_data.clone() #self.org_dataset[0].to(device)\n",
        "    # prepare placeholders\n",
        "    # building masks for training-validation-test sets \n",
        "    unmask = torch.zeros(self.B, dtype=torch.bool).to(device)    \n",
        "    \n",
        "    aug_y = torch.zeros(self.B).type(torch.int64)\n",
        "\n",
        "    aug_x = self.aug_data.x.clone()\n",
        "    aug_nodes_features = torch.zeros_like(torch.empty(self.B, aug_x.shape[1]))\n",
        "\n",
        "    # prepare masks \n",
        "    mask = unmask == 0\n",
        "    if generate_label:\n",
        "      train_mask = self.aug_data.train_mask.clone()\n",
        "      # add augmented nodes to train set only if generate_label=true\n",
        "      train_mask = torch.cat((train_mask, mask), -1)\n",
        "    else:\n",
        "      train_mask = self.aug_data.train_mask.clone()\n",
        "      train_mask = torch.cat((train_mask, unmask), -1)\n",
        "\n",
        "    test_mask = self.aug_data.test_mask.clone()\n",
        "    test_mask = torch.cat((test_mask, unmask), -1)\n",
        "\n",
        "    val_mask = self.aug_data.val_mask.clone()\n",
        "    val_mask = torch.cat((val_mask, unmask), -1)\n",
        "\n",
        "    self.aug_data.train_mask = train_mask\n",
        "    self.aug_data.test_mask = test_mask\n",
        "    self.aug_data.val_mask = val_mask\n",
        "\n",
        "    print(f\"\"\"\n",
        "      Size of augmented training set: {self.aug_data.train_mask.sum()}, size of original training set: {self.org_data.train_mask.sum()}; \n",
        "      Size of augmented test set: {self.aug_data.test_mask.sum()}, size of original test set: {self.org_data.test_mask.sum()};\n",
        "      Size of augmented validation set: {self.aug_data.val_mask.sum()}, size of original validation set: {self.org_data.val_mask.sum()}.\n",
        "    \"\"\")\n",
        "\n",
        "    # prepare edge index\n",
        "    self.aug_data.edge_index = self.aug_g.nonzero().t().contiguous()\n",
        "\n",
        "    # prepare y [not in use]\n",
        "    # add 0's as labels to all augmented nodes [will be masked]\n",
        "    #self.aug_data.y = torch.cat((self.aug_data.y, aug_y.to(device)), -1)\n",
        "    self.aug_data.y = self.gmm_lbl\n",
        "\n",
        "    # prepare features\n",
        "\n",
        "    # make 0's for all augmented nodes in original feature space \n",
        "    aug_x = torch.cat((aug_x, aug_nodes_features.to(device)), 0)\n",
        "        \n",
        "    # make 0's for all all nodes in new feature space \n",
        "    all_nodes_features = torch.zeros_like(torch.empty(aug_x.shape[0], self.B))\n",
        "    aug_x = torch.cat((aug_x, all_nodes_features.to(device)), 1)\n",
        "\n",
        "    # add identity matrix to the augmeneted nodes in new feature space \n",
        "    aug_x[self.org_data.x.shape[0]:, self.org_data.x.shape[1]:] = torch.eye(self.B)\n",
        "\n",
        "    self.aug_data.x = aug_x\n",
        "\n",
        "    # double check the nodes size \n",
        "    assert aug_x.shape[0] == self.aug_data.num_nodes\n",
        "    del unmask, mask, train_mask, test_mask, val_mask, aug_y, aug_x\n",
        "    self.aug_data = self.aug_data.to(device)\n",
        "\n",
        "    return(self.aug_data, self.org_data)\n",
        "    \n",
        "  def train_gnn(self, gnn_type='GAT', lr=0.005, weight_decay=5e-4, epochs=200):\n",
        "    if gnn_type == 'GAT':\n",
        "      self.model = GAT(\n",
        "          num_node_features = self.aug_data.num_node_features, \n",
        "          num_classes=self.num_classes).to(device)\n",
        "    elif gnn_type == 'GCN':\n",
        "      self.model = GCN(\n",
        "          num_node_features = self.aug_data.num_node_features, \n",
        "          num_classes=self.num_classes).to(device)\n",
        "    elif gnn_type == 'GraphSAGE':\n",
        "      self.model = GraphSAGE(\n",
        "          num_node_features = self.aug_data.num_node_features, \n",
        "          num_classes=self.num_classes).to(device)\n",
        "    else: \n",
        "      warnings.simplefilter(\"The requested method has not been implemented.\")\n",
        "\n",
        "    print(f'Number of training nodes: {self.aug_data.train_mask.sum()}')\n",
        "    print(f'Number of validation nodes: {self.aug_data.val_mask.sum()}')\n",
        "    print(f'Number of test nodes: {self.aug_data.test_mask.sum()}')\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        self.model.parameters(), \n",
        "        lr=lr, \n",
        "        weight_decay=weight_decay\n",
        "        )\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        self.model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = self.model(self.aug_data)\n",
        "        #print(f'out={out.shape}')\n",
        "        #loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
        "        #print(f'out[self.aug_data.train_mask]={out[self.aug_data.train_mask].shape}')\n",
        "        #print(f'self.aug_data.y[self.aug_data.train_mask]={self.aug_data.y[self.aug_data.train_mask].shape}')\n",
        "        loss = F.nll_loss(out[self.aug_data.train_mask], self.aug_data.y[self.aug_data.train_mask])\n",
        "        losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(loss)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # print model evaluation \n",
        "    self.model.eval()\n",
        "    _, pred = self.model(self.aug_data).max(dim=1)\n",
        "    correct = float(pred[self.aug_data.train_mask].eq(self.aug_data.y[self.aug_data.train_mask]).sum().item())\n",
        "    self.train_acc = correct / self.aug_data.train_mask.sum().item()\n",
        "    print('Training Set Accuracy: {:.4f}'.format(self.train_acc))\n",
        "\n",
        "    correct = float(pred[self.aug_data.test_mask].eq(self.aug_data.y[self.aug_data.test_mask]).sum().item())\n",
        "    self.test_acc = correct / self.aug_data.test_mask.sum().item()\n",
        "    print('Test Set Accuracy: {:.4f}'.format(self.test_acc))\n",
        "\n",
        "  def _get_graph_info(self):\n",
        "    num_edges = len(self.aug_data.edge_index[0])\n",
        "    train_data_size = self.aug_data.train_mask.sum().item()\n",
        "    test_data_size = self.aug_data.test_mask.sum().item()\n",
        "    val_data_size = self.aug_data.val_mask.sum().item()\n",
        "    num_nodes = self.aug_data.x.shape[0]\n",
        "    num_features = self.aug_data.x.shape[1]\n",
        "\n",
        "    return(self.train_acc, self.test_acc, num_edges, train_data_size, test_data_size, val_data_size, num_nodes, num_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUWFVIWKcXSU"
      },
      "source": [
        "# ⛳️ GMMDA Data Augmentation and Downstream Node Classification "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lOOUcWSIW0z"
      },
      "outputs": [],
      "source": [
        "from torch_geometric import datasets\n",
        "from torch_geometric.utils.convert import to_scipy_sparse_matrix\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "import pickle\n",
        "\n",
        "print(f'The loading dataset is {ds}')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# visualization settings \n",
        "font = {'size'   : 13}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "# GNN settings \n",
        "gnn_type = 'GCN' #'GCN', 'GraphSAGE'\n",
        "num_trails = 5\n",
        "prop_B = 0.07\n",
        "num_candidates = 1 #200 #500\n",
        "\n",
        "with open(MODEL_DIR + f'/{gnn_type}_aux_gmm_v2', 'rb') as fp:\n",
        "    aux_gmm = pickle.load(fp)\n",
        "\n",
        "dataCenter = DataCenter()\n",
        "dataCenter.load_dataSet(ds)\n",
        "\n",
        "# create save directory \n",
        "temp_dir = EXP_RESULT_DIR + f'/{gnn_type}_B%={prop_B}_C={num_candidates}_trails={num_trails}'\n",
        "print(f\"[*] Experiments of B%={prop_B} C={num_candidates} #trails={num_trails} [{gnn_type}] will be saved at {temp_dir}\")\n",
        "!mkdir -p $temp_dir\n",
        "\n",
        "temp_model_dir = temp_dir + f'/model'\n",
        "!mkdir -p $temp_model_dir\n",
        "\n",
        "# load preprocessed data\n",
        "with open(f\"/content/drive/MyDrive/gda_dnml/{ds}/data/{ds}-preprocessed-data\", 'rb') as fp:\n",
        "    data = pickle.load(fp)\n",
        "\n",
        "with open(f\"/content/drive/MyDrive/gda_dnml/{ds}/data/{ds}-preprocessed-dataset\", 'rb') as fp:\n",
        "    dataset = pickle.load(fp)\n",
        "\n",
        "num_classes = dataset.num_classes\n",
        "\n",
        "adj_matrix = to_scipy_sparse_matrix(data.edge_index).tocsr()\n",
        "raw_features = data.x.to('cpu').detach().numpy().copy()\n",
        "labels = data.y.to('cpu').detach().numpy().copy()\n",
        "\n",
        "# number of true classes\n",
        "k_hat = len(np.unique(labels))\n",
        "# all nodes indices \n",
        "nodes = np.arange(0, len(labels), 1) \n",
        "\n",
        "# make placeholders \n",
        "lst_embs_aux_gmm = []\n",
        "lst_best_emb = []\n",
        "lst_gmm_lbl = []\n",
        "\n",
        "df_evaluation_no_aug_label = pd.DataFrame(columns=[\n",
        "    'train_acc', \n",
        "    'test_acc', \n",
        "    'num_nodes', \n",
        "    'num_edges', \n",
        "    'train_data_size', \n",
        "    'test_data_size',\n",
        "    'val_data_size',\n",
        "    'num_features'])\n",
        "\n",
        "df_evaluation_aug_label = pd.DataFrame(columns=[\n",
        "    'train_acc', \n",
        "    'test_acc', \n",
        "    'num_nodes', \n",
        "    'num_edges', \n",
        "    'train_data_size', \n",
        "    'test_data_size',\n",
        "    'val_data_size',\n",
        "    'num_features'])\n",
        "\n",
        "\n",
        "for i in range(num_trails):\n",
        "  # free up some space \n",
        "  if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "\n",
        "  # retrain model \n",
        "  \"\"\"if i % 15 == 0: \n",
        "    with open(f\"/content/drive/MyDrive/gda_dnml/{ds}/data/{ds}-preprocessed-data\", 'rb') as fp:\n",
        "        data = pickle.load(fp)\n",
        "\n",
        "    with open(f\"/content/drive/MyDrive/gda_dnml/{ds}/data/{ds}-preprocessed-dataset\", 'rb') as fp:\n",
        "        dataset = pickle.load(fp)\n",
        "\n",
        "    adj_matrix = to_scipy_sparse_matrix(data.edge_index).tocsr()\n",
        "    raw_features = data.x.to('cpu').detach().numpy().copy()\n",
        "    labels = data.y.to('cpu').detach().numpy().copy()\n",
        "\n",
        "    # start training \n",
        "    aux_gmm = GNN(\n",
        "        adj_matrix, \n",
        "        features=raw_features, \n",
        "        labels=labels, \n",
        "        batch_size=raw_features.shape[0], #len(train_nodes)-1, #len(labels)-1, \n",
        "        emb_size = 64,\n",
        "        epochs = 5, \n",
        "        learn_method='aux', \n",
        "        model=gnn_type.lower(), \n",
        "        print_progress=True\n",
        "    )\n",
        "\n",
        "    # train the model\n",
        "    aux_gmm.fit()\n",
        "\n",
        "    #save to local \n",
        "    with open(temp_model_dir + f'/{gnn_type}_aux_gmm_v2_itr={i}', 'wb') as fp:\n",
        "      pickle.dump(aux_gmm, fp)\"\"\"\n",
        "\n",
        "  embs_aux_gmm = aux_gmm.generate_embeddings(nodes)\n",
        "  lst_embs_aux_gmm.append(embs_aux_gmm)\n",
        "\n",
        "  #save to local \n",
        "  with open(temp_dir + f'/{gnn_type}_lst_embs_aux_gmm', 'wb') as fp:\n",
        "    pickle.dump(lst_embs_aux_gmm, fp)\n",
        "\n",
        "  # conduct embedding selection based on DNML criteria\n",
        "  es = EmbeddingsSelection(\n",
        "      emb=embs_aux_gmm, \n",
        "      prop_B=prop_B,\n",
        "      num_classes=dataset.num_classes, \n",
        "      true_labels=labels,\n",
        "      num_candidates=num_candidates\n",
        "  )\n",
        "  best_emb, gmm_lbl, B = es.compute_dnml()\n",
        "\n",
        "  lst_best_emb.append(best_emb)\n",
        "  lst_gmm_lbl.append(gmm_lbl)\n",
        "\n",
        "  #save to local \n",
        "  with open(temp_dir + f'/{gnn_type}_lst_best_emb', 'wb') as fp:\n",
        "    pickle.dump(lst_best_emb, fp)\n",
        "\n",
        "  with open(temp_dir + f'/{gnn_type}_lst_gmm_lbl', 'wb') as fp:\n",
        "    pickle.dump(lst_gmm_lbl, fp)\n",
        "\n",
        "  #fig = es._plot_tsne()\n",
        "  #fig.savefig(tsne_dnml_dir + f'/TSNE_of_C={num_candidates}_Bpctg={prop_B}_2.png')\n",
        "\n",
        "  # performance of no-augmented-class-label \n",
        "  nc = NodeClassification(\n",
        "      B=B, \n",
        "      emb=best_emb, \n",
        "      ds=ds,\n",
        "      gmm_lbl=gmm_lbl\n",
        "      ) # set threshold >1 to get original graph performance #0.72\n",
        "  #nc._plot_adjacencies()\n",
        "  aug_data_no_aug_label, _ = nc._data_prepare(generate_label=False)\n",
        "  nc.train_gnn(gnn_type=gnn_type)\n",
        "  #aug_data_no_aug_label\n",
        "\n",
        "  train_acc, test_acc, num_edges, train_data_size, \\\n",
        "  test_data_size, val_data_size, num_nodes, num_features = nc._get_graph_info()\n",
        "\n",
        "  df_evaluation_no_aug_label = df_evaluation_no_aug_label.append({\n",
        "    'train_acc': train_acc, \n",
        "    'test_acc': test_acc, \n",
        "    'num_nodes': num_nodes, \n",
        "    'num_edges': num_edges, \n",
        "    'train_data_size': train_data_size, \n",
        "    'test_data_size': test_data_size,\n",
        "    'val_data_size': val_data_size,\n",
        "    'num_features': num_features}, ignore_index=True)\n",
        "\n",
        "  df_evaluation_no_aug_label.to_csv(temp_dir+f'/df_evaluation_no_aug_label.csv', index=True)\n",
        "\n",
        "  # performance of augmented-class-label \n",
        "  aug_data_aug_label, _ = nc._data_prepare(generate_label=True)\n",
        "  nc.train_gnn(gnn_type=gnn_type)\n",
        "\n",
        "  train_acc, test_acc, num_edges, train_data_size, \\\n",
        "  test_data_size, val_data_size, num_nodes, num_features = nc._get_graph_info()\n",
        "\n",
        "  df_evaluation_aug_label = df_evaluation_aug_label.append({\n",
        "    'train_acc': train_acc, \n",
        "    'test_acc': test_acc, \n",
        "    'num_nodes': num_nodes, \n",
        "    'num_edges': num_edges, \n",
        "    'train_data_size': train_data_size, \n",
        "    'test_data_size': test_data_size,\n",
        "    'val_data_size': val_data_size,\n",
        "    'num_features': num_features}, ignore_index=True)\n",
        "\n",
        "  df_evaluation_aug_label.to_csv(temp_dir+'/df_evaluation_aug_label.csv', index=True)\n",
        "\n",
        "\n",
        "  #aug_data_aug_label"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO2keEDy0YN4vo2SuAaNWpR",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}